"""
Azure OpenAI DJ Voice Service - Uses Azure OpenAI for creative DJ commentary and TTS.
Replaces edge-tts with Azure OpenAI's gpt-4o-mini-audio-preview for high-quality voice.
Uses GPT-4 to generate creative, contextual DJ commentary based on user theme and song metadata.
"""

import os
import base64
import logging
import subprocess
import tempfile
import json
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Tuple
import sys

logger = logging.getLogger(__name__)

def log(msg: str):
    """Print with immediate flush for logging."""
    print(msg, flush=True)
    sys.stdout.flush()

# Azure OpenAI configuration - Using AAD authentication (DefaultAzureCredential)
AZURE_OPENAI_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT", "https://aidj-openai.openai.azure.com/")
AZURE_OPENAI_DEPLOYMENT = os.environ.get("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
AZURE_OPENAI_AUDIO_DEPLOYMENT = os.environ.get("AZURE_OPENAI_AUDIO_DEPLOYMENT", "gpt-4o-mini-audio")

# Check if Azure OpenAI is configured (using AAD auth - no API key needed!)
AZURE_OPENAI_AVAILABLE = bool(AZURE_OPENAI_ENDPOINT)
_azure_credential = None
_AzureOpenAI = None

if AZURE_OPENAI_AVAILABLE:
    try:
        from openai import AzureOpenAI as _AzureOpenAI
        from azure.identity import DefaultAzureCredential, get_bearer_token_provider
        _azure_credential = DefaultAzureCredential()
        log("[AZURE_DJ] Azure OpenAI configured with AAD authentication (DefaultAzureCredential)")
    except ImportError as e:
        AZURE_OPENAI_AVAILABLE = False
        log(f"[AZURE_DJ] Required packages not installed: {e}")
        log("[AZURE_DJ] Run: pip install openai azure-identity")
else:
    log("[AZURE_DJ] Azure OpenAI not configured (missing AZURE_OPENAI_ENDPOINT)")

# Fallback to edge-tts if Azure OpenAI is not available
try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
except ImportError:
    EDGE_TTS_AVAILABLE = False

# DJ Voice options for Azure OpenAI (Alloy, Echo, Shimmer are supported)
AZURE_DJ_VOICES = {
    "energetic_male": "echo",      # Echo has an energetic male quality
    "energetic_female": "shimmer", # Shimmer is energetic female
    "deep_male": "echo",           # Echo for deep male (adjust with prompting)
    "party_female": "alloy",       # Alloy is versatile
    "hype_male": "echo",           # Echo for hype
}

# Language metadata for creative commentary
LANGUAGE_INFO = {
    "english": {"country": "worldwide", "vibe": "global hits", "artists": ["Ed Sheeran", "Taylor Swift", "Bruno Mars"]},
    "hindi": {"country": "India", "vibe": "Bollywood magic", "artists": ["Arijit Singh", "Shreya Ghoshal", "SRK movies"]},
    "malayalam": {"country": "Kerala", "vibe": "Mollywood melodies", "artists": ["Mohanlal", "Mammootty", "Dulquer"]},
    "tamil": {"country": "Tamil Nadu", "vibe": "Kollywood beats", "artists": ["Rajinikanth", "Vijay", "AR Rahman"]},
    "turkish": {"country": "Turkey", "vibe": "Turkish pop vibes", "artists": ["Tarkan", "Sezen Aksu"]},
    "uzbek": {"country": "Uzbekistan", "vibe": "Central Asian rhythms", "artists": ["Uzbek folk fusion"]},
    "arabic": {"country": "Middle East", "vibe": "Arabic grooves", "artists": ["Amr Diab", "Nancy Ajram"]},
}


@dataclass
class DJContext:
    """User-provided context for DJ commentary."""
    theme: str = "New Year 2025 Party - Welcoming 2026!"
    mood: str = "energetic, celebratory, festive"  # Can be string or list
    audience: str = "party guests ready to dance"
    special_notes: str = ""
    custom_shoutouts: List[str] = field(default_factory=list)
    original_prompt: str = ""  # The original user prompt for reference
    
    def get_mood_str(self) -> str:
        """Get mood as a string, handling both string and list inputs."""
        if isinstance(self.mood, list):
            return ", ".join(self.mood)
        return self.mood


@dataclass
class SongMetadata:
    """Extracted metadata about a song for DJ commentary."""
    title: str
    artist: Optional[str] = None
    language: str = "english"
    bpm: Optional[float] = None
    energy_score: float = 0.5
    movie_or_album: Optional[str] = None
    famous_actors: List[str] = field(default_factory=list)
    position: int = 0


@dataclass
class CreativeDJComment:
    """A creative DJ comment generated by GPT."""
    text: str
    comment_type: str  # intro, hype, transition, language_switch, song_intro, outro
    position: str  # before, after, between
    segment_index: int
    audio_path: Optional[str] = None
    voice_style: str = "energetic"


def get_azure_openai_client():
    """Get Azure OpenAI client if available, using AAD authentication."""
    global _azure_credential, _AzureOpenAI
    
    if not AZURE_OPENAI_AVAILABLE or not _AzureOpenAI or not _azure_credential:
        return None
    
    try:
        from azure.identity import get_bearer_token_provider
        token_provider = get_bearer_token_provider(
            _azure_credential,
            "https://cognitiveservices.azure.com/.default"
        )
        
        client = _AzureOpenAI(
            api_version="2025-01-01-preview",
            azure_endpoint=AZURE_OPENAI_ENDPOINT,
            azure_ad_token_provider=token_provider
        )
        return client
    except Exception as e:
        log(f"[AZURE_DJ] Failed to create Azure OpenAI client: {e}")
        return None


def extract_song_metadata(song_info: Dict) -> SongMetadata:
    """Extract useful metadata from song info for creative commentary."""
    title = song_info.get("song_title", "Unknown Track")
    artist = song_info.get("artist")
    language = song_info.get("language", "english").lower()
    bpm = song_info.get("bpm")
    energy = song_info.get("energy_score", 0.5)
    position = song_info.get("position", 0)
    
    # Try to extract movie/actor info from title
    movie_or_album = None
    famous_actors = []
    
    # Common patterns in Bollywood/Indian movie song titles
    title_lower = title.lower()
    
    # Check for common actors/stars in title
    star_keywords = {
        "srk": "Shah Rukh Khan",
        "shah rukh": "Shah Rukh Khan",
        "shahrukh": "Shah Rukh Khan",
        "salman": "Salman Khan",
        "aamir": "Aamir Khan",
        "ranveer": "Ranveer Singh",
        "ranbir": "Ranbir Kapoor",
        "hrithik": "Hrithik Roshan",
        "deepika": "Deepika Padukone",
        "alia": "Alia Bhatt",
        "priyanka": "Priyanka Chopra",
        "rajini": "Rajinikanth",
        "vijay": "Vijay",
        "mohanlal": "Mohanlal",
        "mammootty": "Mammootty",
    }
    
    for keyword, star_name in star_keywords.items():
        if keyword in title_lower:
            famous_actors.append(star_name)
    
    return SongMetadata(
        title=title,
        artist=artist,
        language=language,
        bpm=bpm,
        energy_score=energy,
        movie_or_album=movie_or_album,
        famous_actors=famous_actors,
        position=position
    )


def generate_creative_commentary_with_gpt(
    segments: List[Dict],
    context: DJContext,
    frequency: str = "moderate"
) -> List[CreativeDJComment]:
    """
    Use Azure OpenAI GPT to generate creative, contextual DJ commentary.
    
    Args:
        segments: List of segment info with song metadata
        context: User-provided DJ context (theme, mood, etc.)
        frequency: How often to add comments (minimal, moderate, frequent)
    
    Returns:
        List of creative DJ comments
    """
    client = get_azure_openai_client()
    
    if not client:
        log("[AZURE_DJ] GPT not available, using fallback commentary")
        return generate_fallback_commentary(segments, context)
    
    # Build song list description - human-friendly, no technical details
    songs_desc = []
    for i, seg in enumerate(segments):
        meta = extract_song_metadata(seg)
        lang_info = LANGUAGE_INFO.get(meta.language, {"country": "worldwide", "vibe": "great music"})
        
        song_line = f"{i+1}. \"{meta.title}\" ({meta.language.title()}"
        if meta.artist:
            song_line += f", by {meta.artist}"
        # Skip BPM - keep it human-like, not technical
        if meta.famous_actors:
            song_line += f", featuring: {', '.join(meta.famous_actors)}"
        song_line += ")"
        songs_desc.append(song_line)
    
    songs_list = "\n".join(songs_desc)
    
    # Determine number of comments based on frequency
    comment_counts = {
        "minimal": 2,      # intro, outro only
        "moderate": 4,     # intro, 1-2 transitions, outro
        "frequent": 5,     # intro, transitions, outro
        "maximum": 7       # More comments
    }
    num_comments = comment_counts.get(frequency, 4)
    
    # Build the prompt
    mood_str = context.get_mood_str() if hasattr(context, 'get_mood_str') else context.mood
    
    prompt = f"""You are an energetic AI DJ at a party! Generate {num_comments} DJ voice-over comments for a video DJ mix.

PARTY THEME: {context.theme}
MOOD: {mood_str}
AUDIENCE: {context.audience}
{f"SPECIAL NOTES: {context.special_notes}" if context.special_notes else ""}
{f"ORIGINAL REQUEST: {context.original_prompt}" if context.original_prompt else ""}
{f"SHOUTOUTS TO INCLUDE: {', '.join(context.custom_shoutouts)}" if context.custom_shoutouts else ""}

PLAYLIST ORDER ({len(segments)} songs - this is the exact order they will play):
{songs_list}

CRITICAL RULES:
1. **segment_index MUST match the song you're talking about!**
   - segment_index 0 = Song 1 (intro plays during/before song 1)
   - segment_index 1 = Song 2 
   - segment_index 2 = Song 3, etc.
2. When you say "song_intro" for segment_index N, you MUST reference song N+1's title/artist
3. When you say "transition" to segment_index N, you're transitioning TO song N+1
4. "hype" or "peak_energy" for segment_index N should hype the CURRENT song N+1
5. **NEVER mention a song in the wrong position** - check the playlist order above!

INSTRUCTIONS:
1. Create exactly {num_comments} DJ comments as JSON array
2. Be creative but BRIEF - real DJs don't ramble!
3. Reference the party theme naturally
4. For Bollywood, quick nods like "SRK vibes!" or "Kollywood heat!"
5. Quick language shoutouts: "Arey!", "Let's go!", "Thalaivar!"
6. Be NATURAL - don't over-explain or list song details
7. **KEEP IT SHORT: 5-12 words MAX per comment (3-5 seconds to speak)**
8. Punchy energy, not long speeches!

OUTPUT FORMAT (JSON array) - segment_index 0 = first song, 1 = second song, etc:
[
  {{"type": "intro", "text": "Welcome/theme intro", "segment_index": 0}},
  {{"type": "song_intro", "text": "Intro for song 1", "segment_index": 0}},
  {{"type": "transition", "text": "Transition to song 2", "segment_index": 1}},
  {{"type": "song_intro", "text": "Intro for song 2", "segment_index": 1}},
  ...
  {{"type": "outro", "text": "Closing comment", "segment_index": {len(segments)-1}}}
]

Types: intro (party opener), song_intro (introduce specific song), transition (bridge to next song), hype (energy boost), peak_energy (climax moment), outro (closing)

Generate the JSON array now:"""

    try:
        log(f"[AZURE_DJ] Generating {num_comments} creative comments with GPT...")
        
        response = client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "You are an energetic party DJ. Output only valid JSON arrays."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.9,  # High creativity
            max_tokens=1500
        )
        
        content = response.choices[0].message.content.strip()
        
        # Parse JSON from response
        # Sometimes GPT adds markdown code blocks
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        content = content.strip()
        
        comments_data = json.loads(content)
        
        comments = []
        for item in comments_data:
            comment = CreativeDJComment(
                text=item.get("text", ""),
                comment_type=item.get("type", "hype"),
                position="between" if item.get("type") not in ["intro", "outro"] else ("before" if item.get("type") == "intro" else "after"),
                segment_index=item.get("segment_index", 0)
            )
            comments.append(comment)
        
        log(f"[AZURE_DJ] Generated {len(comments)} creative comments!")
        for c in comments:
            log(f"[AZURE_DJ]   [{c.comment_type}] {c.text[:50]}...")
        
        return comments
        
    except json.JSONDecodeError as e:
        log(f"[AZURE_DJ] Failed to parse GPT response as JSON: {e}")
        log(f"[AZURE_DJ] Response was: {content[:500]}")
        return generate_fallback_commentary(segments, context)
    except Exception as e:
        log(f"[AZURE_DJ] GPT generation failed: {e}")
        return generate_fallback_commentary(segments, context)


def generate_fallback_commentary(
    segments: List[Dict],
    context: DJContext
) -> List[CreativeDJComment]:
    """Fallback commentary when GPT is not available."""
    comments = []
    
    # Intro
    intro_text = f"What's up party people! Welcome to the {context.theme}! Let's get this celebration started!"
    comments.append(CreativeDJComment(
        text=intro_text,
        comment_type="intro",
        position="before",
        segment_index=0
    ))
    
    # Middle comment
    if len(segments) > 1:
        mid_idx = len(segments) // 2
        mid_lang = segments[mid_idx].get("language", "english")
        mid_text = f"We're halfway through this amazing mix! {mid_lang.title()} vibes coming in hot!"
        comments.append(CreativeDJComment(
            text=mid_text,
            comment_type="transition",
            position="between",
            segment_index=mid_idx
        ))
    
    # Outro
    outro_text = f"That's a wrap on this incredible party! {context.theme} - what a night! Stay groovy!"
    comments.append(CreativeDJComment(
        text=outro_text,
        comment_type="outro",
        position="after",
        segment_index=len(segments) - 1
    ))
    
    return comments


async def generate_voice_clip_azure_async(
    text: str,
    output_path: Path,
    voice: str = "alloy"
) -> bool:
    """Generate a voice clip using Azure OpenAI gpt-4o-mini-audio-preview."""
    client = get_azure_openai_client()
    
    if not client:
        log("[AZURE_DJ] Azure OpenAI client not available for TTS")
        return False
    
    azure_voice = AZURE_DJ_VOICES.get(voice, "alloy")
    
    try:
        log(f"[AZURE_DJ] Generating voice with Azure OpenAI ({azure_voice})...")
        
        response = client.chat.completions.create(
            model=AZURE_OPENAI_AUDIO_DEPLOYMENT,  # gpt-4o-mini-audio-preview
            modalities=["text", "audio"],
            audio={"voice": azure_voice, "format": "wav"},
            messages=[
                {
                    "role": "system",
                    "content": "You are an energetic party DJ. Speak with enthusiasm and energy!"
                },
                {
                    "role": "user",
                    "content": f"Read this DJ announcement with high energy: {text}"
                }
            ]
        )
        
        # Extract audio data from response
        if response.choices[0].message.audio:
            audio_data = response.choices[0].message.audio.data
            wav_bytes = base64.b64decode(audio_data)
            
            with open(output_path, "wb") as f:
                f.write(wav_bytes)
            
            log(f"[AZURE_DJ] Generated voice clip: {output_path} ({len(wav_bytes)} bytes)")
            return True
        else:
            log("[AZURE_DJ] No audio data in response")
            return False
            
    except Exception as e:
        log(f"[AZURE_DJ] Azure OpenAI TTS failed: {e}")
        return False


def generate_voice_clip_azure(
    text: str,
    output_path: Path,
    voice: str = "alloy"
) -> bool:
    """Synchronous wrapper for Azure voice generation."""
    import asyncio
    
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(generate_voice_clip_azure_async(text, output_path, voice))
    
    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor() as pool:
        future = pool.submit(asyncio.run, generate_voice_clip_azure_async(text, output_path, voice))
        return future.result()


async def generate_voice_clip_edge_tts_async(
    text: str,
    output_path: Path,
    voice_id: str = "en-US-GuyNeural"
) -> bool:
    """Fallback to edge-tts for voice generation."""
    if not EDGE_TTS_AVAILABLE:
        return False
    
    try:
        communicate = edge_tts.Communicate(text, voice_id)
        await communicate.save(str(output_path))
        return True
    except Exception as e:
        log(f"[AZURE_DJ] edge-tts failed: {e}")
        return False


def generate_voice_clip_edge_tts(
    text: str,
    output_path: Path,
    voice_id: str = "en-US-GuyNeural"
) -> bool:
    """Synchronous wrapper for edge-tts."""
    import asyncio
    
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(generate_voice_clip_edge_tts_async(text, output_path, voice_id))
    
    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor() as pool:
        future = pool.submit(asyncio.run, generate_voice_clip_edge_tts_async(text, output_path, voice_id))
        return future.result()


def generate_voice_clip(
    text: str,
    output_path: Path,
    voice: str = "energetic_male"
) -> bool:
    """Generate voice clip using Azure OpenAI or fallback to edge-tts."""
    
    # Try Azure OpenAI first
    if AZURE_OPENAI_AVAILABLE:
        success = generate_voice_clip_azure(text, output_path, voice)
        if success:
            return True
        log("[AZURE_DJ] Azure OpenAI TTS failed, trying fallback...")
    
    # Fallback to edge-tts
    if EDGE_TTS_AVAILABLE:
        edge_voice_map = {
            "energetic_male": "en-US-GuyNeural",
            "energetic_female": "en-US-AriaNeural",
            "deep_male": "en-US-ChristopherNeural",
            "party_female": "en-US-JennyNeural",
            "hype_male": "en-GB-RyanNeural",
        }
        voice_id = edge_voice_map.get(voice, "en-US-GuyNeural")
        return generate_voice_clip_edge_tts(text, output_path, voice_id)
    
    log("[AZURE_DJ] No TTS engine available!")
    return False


def get_dj_clip_duration(audio_path: str) -> float:
    """Get duration of a DJ audio clip."""
    try:
        cmd = [
            'ffprobe', '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            audio_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            return float(result.stdout.strip())
    except:
        pass
    return 2.0  # Default estimate


def get_stream_durations(video_path) -> Tuple[float, float]:
    """Get both video and audio stream durations separately."""
    cmd = [
        'ffprobe', '-v', 'error',
        '-show_entries', 'stream=codec_type,duration',
        '-of', 'json',
        str(video_path)
    ]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            data = json.loads(result.stdout)
            video_dur = 0
            audio_dur = 0
            for stream in data.get("streams", []):
                if stream.get("codec_type") == "video":
                    video_dur = float(stream.get("duration", 0))
                elif stream.get("codec_type") == "audio":
                    audio_dur = float(stream.get("duration", 0))
            return video_dur, audio_dur
    except Exception as e:
        log(f"[AZURE_DJ] Could not get stream durations: {e}")
    return 0, 0


def add_creative_dj_commentary_to_video(
    video_path: Path,
    segments: List[Dict],
    output_path: Path,
    context: DJContext,
    voice: str = "energetic_male",
    frequency: str = "moderate"
) -> Tuple[bool, List[Dict]]:
    """
    Complete creative DJ voice integration using Azure OpenAI.
    
    Args:
        video_path: Input video file
        segments: List of segment info with song metadata
        output_path: Output video file
        context: User-provided DJ context
        voice: Voice style to use
        frequency: Comment frequency
    
    Returns:
        Tuple of (success: bool, timeline: list of timing info)
    """
    timeline = []
    
    log("=" * 60)
    log("[AZURE_DJ] ADD_CREATIVE_DJ_COMMENTARY_TO_VIDEO STARTED")
    log(f"[AZURE_DJ] Theme: {context.theme}")
    log(f"[AZURE_DJ] Mood: {context.mood}")
    log(f"[AZURE_DJ] Voice: {voice}, Frequency: {frequency}")
    log(f"[AZURE_DJ] Segments: {len(segments)}")
    log(f"[AZURE_DJ] Azure OpenAI Available: {AZURE_OPENAI_AVAILABLE}")
    log("=" * 60)
    
    if not AZURE_OPENAI_AVAILABLE and not EDGE_TTS_AVAILABLE:
        log("[AZURE_DJ] No TTS engine available!")
        return False, timeline
    
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # Get video duration
        video_dur, audio_dur = get_stream_durations(video_path)
        video_duration = min(video_dur, audio_dur) if video_dur > 0 and audio_dur > 0 else max(video_dur, audio_dur)
        
        if video_duration <= 0:
            video_duration = get_dj_clip_duration(str(video_path))
        
        log(f"[AZURE_DJ] Video duration: {video_duration:.2f}s")
        
        # Generate creative commentary using GPT
        comments = generate_creative_commentary_with_gpt(segments, context, frequency)
        
        if not comments:
            log("[AZURE_DJ] No comments generated!")
            return False, timeline
        
        # Build segment timing map - use actual video_start_time if available
        # This tells us exactly when each song starts in the final video
        num_segments = len(segments)
        segment_timings = []  # (start_time_in_video, duration, song_title)
        
        for i, seg in enumerate(segments):
            if 'video_start_time' in seg:
                # Use actual timing from exporter
                start = seg['video_start_time']
                duration = seg.get('segment_duration', 60)
            else:
                # Fallback: estimate based on position (4s intro + segments)
                avg_seg_duration = (video_duration - 4 - 3) / num_segments if num_segments > 0 else 60
                start = 4.0 + (i * avg_seg_duration)
                duration = avg_seg_duration
            
            segment_timings.append({
                'start': start,
                'duration': duration,
                'title': seg.get('song_title', seg.get('title', 'Unknown')),
                'artist': seg.get('artist', '')
            })
        
        log(f"[AZURE_DJ] Segment timings in final video:")
        for i, st in enumerate(segment_timings):
            log(f"[AZURE_DJ]   Song {i+1}: {st['title'][:30]} @ {st['start']:.1f}s ({st['duration']:.0f}s)")
        
        dj_clips = []
        # Track last end time to prevent overlaps
        last_clip_end_time = 0.0
        
        for i, comment in enumerate(comments):
            clip_path = temp_dir / f"dj_{i}.wav"
            seg_idx = min(comment.segment_index, num_segments - 1)
            
            # Calculate start time based on comment type using actual segment timings
            if comment.comment_type == "intro":
                start_time = 1.5  # Intro starts 1.5 seconds in (during intro clip)
            elif comment.comment_type == "outro":
                start_time = max(video_duration - 12.0, video_duration * 0.85)
            elif comment.comment_type == "song_intro":
                # Song intro should play RIGHT AT the start of that song
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    # Start DJ comment 2 seconds into the song (after transition settles)
                    start_time = song_start + 2.0
                else:
                    start_time = last_clip_end_time + 5.0
            elif comment.comment_type == "transition":
                # Transition comment plays BEFORE the next song starts (during crossfade)
                if seg_idx < len(segment_timings):
                    next_song_start = segment_timings[seg_idx]['start']
                    # Start 5 seconds before the next song
                    start_time = max(next_song_start - 5.0, last_clip_end_time + 3.0)
                else:
                    start_time = last_clip_end_time + 5.0
            elif comment.comment_type == "hype" or comment.comment_type == "peak_energy":
                # Hype/peak energy comments go in the MIDDLE of the song
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    song_duration = segment_timings[seg_idx]['duration']
                    # Place at 40% into the song
                    start_time = song_start + (song_duration * 0.4)
                else:
                    start_time = last_clip_end_time + 8.0
            else:
                # Other comments - position 30% into the segment
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    song_duration = segment_timings[seg_idx]['duration']
                    start_time = song_start + (song_duration * 0.3)
                else:
                    start_time = last_clip_end_time + 5.0
            
            # Ensure no overlap with previous clip (at least 3s gap)
            if start_time < last_clip_end_time + 3.0:
                start_time = last_clip_end_time + 3.0
            
            # Generate voice clip
            log(f"[AZURE_DJ] Generating clip {i+1}/{len(comments)}: {comment.comment_type}")
            success = generate_voice_clip(comment.text, clip_path, voice)
            
            if success and clip_path.exists():
                clip_duration = get_dj_clip_duration(str(clip_path))
                
                # Ensure clip doesn't extend past video
                if start_time + clip_duration > video_duration:
                    start_time = max(0, video_duration - clip_duration - 2)
                
                # Ensure no overlap with previous clip (at least 2s gap)
                if start_time < last_clip_end_time + 2.0:
                    start_time = last_clip_end_time + 2.0
                
                dj_clips.append({
                    "path": str(clip_path),
                    "start_time": start_time,
                    "duration": clip_duration,
                    "type": comment.comment_type,
                    "text": comment.text,
                })
                timeline.append({
                    "type": comment.comment_type,
                    "start_time": start_time,
                    "end_time": start_time + clip_duration,
                    "text": comment.text[:50] + "..."
                })
                
                # Track end time for next clip
                last_clip_end_time = start_time + clip_duration
                
                log(f"[AZURE_DJ]   ✓ {comment.comment_type} @ {start_time:.1f}s ({clip_duration:.1f}s)")
            else:
                log(f"[AZURE_DJ]   ✗ Failed to generate clip for {comment.comment_type}")
        
        if not dj_clips:
            log("[AZURE_DJ] No DJ clips generated!")
            return False, timeline
        
        # Mix DJ audio with video using FFmpeg
        log(f"[AZURE_DJ] Mixing {len(dj_clips)} clips with video...")
        
        # Build FFmpeg command
        input_args = ['-i', str(video_path)]
        filter_parts = []
        
        # Build duck expression
        duck_expr_parts = []
        for clip in dj_clips:
            start = clip["start_time"]
            end = start + clip["duration"]
            duck_expr_parts.append(f"between(t,{start},{end})")
        
        # Music: duck to 35% during DJ (music still audible but DJ is clear)
        if duck_expr_parts:
            duck_cond = '+'.join(duck_expr_parts)
            filter_parts.append(f"[0:a]volume='if({duck_cond},0.35,1.0)':eval=frame[music]")
        else:
            filter_parts.append("[0:a]anull[music]")
        
        # Add each DJ clip - boost voice to be clear over ducked music
        mix_labels = ['[music]']
        for i, clip in enumerate(dj_clips):
            input_args.extend(['-i', clip["path"]])
            input_idx = i + 1
            delay_ms = int(clip["start_time"] * 1000)
            filter_parts.append(f"[{input_idx}:a]adelay={delay_ms}|{delay_ms},volume=2.0[dj{i}]")
            mix_labels.append(f'[dj{i}]')
        
        # Final mix - balanced weights for music and DJ
        num_inputs = len(mix_labels)
        weights = '1 ' + ' '.join(['2.5'] * (num_inputs - 1))
        filter_parts.append(
            f"{''.join(mix_labels)}amix=inputs={num_inputs}:duration=first:dropout_transition=0:normalize=0:weights='{weights}'[aout]"
        )
        
        filter_complex = ';'.join(filter_parts)
        
        cmd = [
            'ffmpeg', '-y',
            *input_args,
            '-filter_complex', filter_complex,
            '-map', '0:v',
            '-map', '[aout]',
            '-c:v', 'libx264', '-preset', 'fast',
            '-c:a', 'aac', '-b:a', '192k',
            '-vsync', 'cfr', '-r', '30',
            '-shortest',
            str(output_path)
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            log(f"[AZURE_DJ] FFmpeg failed: {result.stderr[:500]}")
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False, timeline
        
        log(f"[AZURE_DJ] Successfully created DJ video: {output_path}")
        log("[AZURE_DJ] === DJ VOICE TIMELINE ===")
        for t in timeline:
            log(f"[AZURE_DJ]   {t['type'].upper():12} @ {t['start_time']:.1f}s - {t['end_time']:.1f}s")
        log("[AZURE_DJ] =========================")
        
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        
        return True, timeline
        
    except Exception as e:
        log(f"[AZURE_DJ] Error: {e}")
        import traceback
        traceback.print_exc()
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        return False, timeline


# Default DJ context for New Year 2025 party
DEFAULT_DJ_CONTEXT = DJContext(
    theme="New Year 2025 Party - Welcoming 2026!",
    mood="energetic, celebratory, festive, countdown vibes",
    audience="party guests ready to dance and celebrate",
    special_notes="Last party of 2025! Let's make it epic!",
    custom_shoutouts=["Happy New Year!", "2026 here we come!"]
)
