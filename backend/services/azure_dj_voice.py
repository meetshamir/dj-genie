"""
Azure OpenAI DJ Voice Service - Uses Azure OpenAI for creative DJ commentary and TTS.
Replaces edge-tts with Azure OpenAI's gpt-4o-mini-audio-preview for high-quality voice.
Uses GPT-4 to generate creative, contextual DJ commentary based on user theme and song metadata.
"""

import os
import base64
import logging
import subprocess
import tempfile
import json
import time
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Tuple
import sys

logger = logging.getLogger(__name__)

def log(msg: str):
    """Print with immediate flush for logging."""
    print(msg, flush=True)
    sys.stdout.flush()

# Azure OpenAI configuration - Using AAD authentication (DefaultAzureCredential)
# Set AZURE_OPENAI_ENDPOINT environment variable to your Azure OpenAI endpoint
AZURE_OPENAI_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT", "")
AZURE_OPENAI_DEPLOYMENT = os.environ.get("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
AZURE_OPENAI_AUDIO_DEPLOYMENT = os.environ.get("AZURE_OPENAI_AUDIO_DEPLOYMENT", "gpt-4o-mini-audio")

# Check if Azure OpenAI is configured (using AAD auth - no API key needed!)
AZURE_OPENAI_AVAILABLE = bool(AZURE_OPENAI_ENDPOINT)
_azure_credential = None
_AzureOpenAI = None

if AZURE_OPENAI_AVAILABLE:
    try:
        from openai import AzureOpenAI as _AzureOpenAI
        from azure.identity import DefaultAzureCredential, get_bearer_token_provider
        _azure_credential = DefaultAzureCredential()
        log("[AZURE_DJ] Azure OpenAI configured with AAD authentication (DefaultAzureCredential)")
    except ImportError as e:
        AZURE_OPENAI_AVAILABLE = False
        log(f"[AZURE_DJ] Required packages not installed: {e}")
        log("[AZURE_DJ] Run: pip install openai azure-identity")
else:
    log("[AZURE_DJ] Azure OpenAI not configured (missing AZURE_OPENAI_ENDPOINT)")

# Fallback to edge-tts if Azure OpenAI is not available
try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
except ImportError:
    EDGE_TTS_AVAILABLE = False

# DJ Voice options for Azure OpenAI (Alloy, Echo, Shimmer are supported)
AZURE_DJ_VOICES = {
    "energetic_male": "echo",      # Echo has an energetic male quality
    "energetic_female": "shimmer", # Shimmer is energetic female
    "deep_male": "echo",           # Echo for deep male (adjust with prompting)
    "party_female": "alloy",       # Alloy is versatile
    "hype_male": "echo",           # Echo for hype
}

# Language metadata for creative commentary
LANGUAGE_INFO = {
    "english": {"country": "worldwide", "vibe": "global hits", "artists": ["Ed Sheeran", "Taylor Swift", "Bruno Mars"]},
    "hindi": {"country": "India", "vibe": "Bollywood magic", "artists": ["Arijit Singh", "Shreya Ghoshal", "SRK movies"]},
    "malayalam": {"country": "Kerala", "vibe": "Mollywood melodies", "artists": ["Mohanlal", "Mammootty", "Dulquer"]},
    "tamil": {"country": "Tamil Nadu", "vibe": "Kollywood beats", "artists": ["Rajinikanth", "Vijay", "AR Rahman"]},
    "turkish": {"country": "Turkey", "vibe": "Turkish pop vibes", "artists": ["Tarkan", "Sezen Aksu"]},
    "uzbek": {"country": "Uzbekistan", "vibe": "Central Asian rhythms", "artists": ["Uzbek folk fusion"]},
    "arabic": {"country": "Middle East", "vibe": "Arabic grooves", "artists": ["Amr Diab", "Nancy Ajram"]},
}


@dataclass
class DJContext:
    """User-provided context for DJ commentary."""
    theme: str = "New Year 2025 Party - Welcoming 2026!"
    mood: str = "energetic, celebratory, festive"  # Can be string or list
    audience: str = "party guests ready to dance"
    special_notes: str = ""
    custom_shoutouts: List[str] = field(default_factory=list)
    original_prompt: str = ""  # The original user prompt for reference
    
    def get_mood_str(self) -> str:
        """Get mood as a string, handling both string and list inputs."""
        if isinstance(self.mood, list):
            return ", ".join(self.mood)
        return self.mood


@dataclass
class SongMetadata:
    """Extracted metadata about a song for DJ commentary."""
    title: str
    artist: Optional[str] = None
    language: str = "english"
    bpm: Optional[float] = None
    energy_score: float = 0.5
    movie_or_album: Optional[str] = None
    famous_actors: List[str] = field(default_factory=list)
    position: int = 0


@dataclass
class CreativeDJComment:
    """A creative DJ comment generated by GPT."""
    text: str
    comment_type: str  # intro, hype, transition, language_switch, song_intro, outro
    position: str  # before, after, between
    segment_index: int
    audio_path: Optional[str] = None
    voice_style: str = "energetic"


def get_azure_openai_client():
    """Get Azure OpenAI client if available, using AAD authentication."""
    global _azure_credential, _AzureOpenAI
    
    if not AZURE_OPENAI_AVAILABLE or not _AzureOpenAI or not _azure_credential:
        return None
    
    try:
        from azure.identity import get_bearer_token_provider
        token_provider = get_bearer_token_provider(
            _azure_credential,
            "https://cognitiveservices.azure.com/.default"
        )
        
        client = _AzureOpenAI(
            api_version="2025-01-01-preview",
            azure_endpoint=AZURE_OPENAI_ENDPOINT,
            azure_ad_token_provider=token_provider
        )
        return client
    except Exception as e:
        log(f"[AZURE_DJ] Failed to create Azure OpenAI client: {e}")
        return None


def extract_song_metadata(song_info: Dict) -> SongMetadata:
    """Extract useful metadata from song info for creative commentary."""
    title = song_info.get("song_title", "Unknown Track")
    artist = song_info.get("artist")
    language = song_info.get("language", "english").lower()
    bpm = song_info.get("bpm")
    energy = song_info.get("energy_score", 0.5)
    position = song_info.get("position", 0)
    
    # Try to extract movie/actor info from title
    movie_or_album = None
    famous_actors = []
    
    # Common patterns in Bollywood/Indian movie song titles
    title_lower = title.lower()
    
    # Check for common actors/stars in title
    star_keywords = {
        "srk": "Shah Rukh Khan",
        "shah rukh": "Shah Rukh Khan",
        "shahrukh": "Shah Rukh Khan",
        "salman": "Salman Khan",
        "aamir": "Aamir Khan",
        "ranveer": "Ranveer Singh",
        "ranbir": "Ranbir Kapoor",
        "hrithik": "Hrithik Roshan",
        "deepika": "Deepika Padukone",
        "alia": "Alia Bhatt",
        "priyanka": "Priyanka Chopra",
        "rajini": "Rajinikanth",
        "vijay": "Vijay",
        "mohanlal": "Mohanlal",
        "mammootty": "Mammootty",
    }
    
    for keyword, star_name in star_keywords.items():
        if keyword in title_lower:
            famous_actors.append(star_name)
    
    return SongMetadata(
        title=title,
        artist=artist,
        language=language,
        bpm=bpm,
        energy_score=energy,
        movie_or_album=movie_or_album,
        famous_actors=famous_actors,
        position=position
    )


def generate_creative_commentary_with_gpt(
    segments: List[Dict],
    context: DJContext,
    frequency: str = "moderate"
) -> List[CreativeDJComment]:
    """
    Use Azure OpenAI GPT to generate creative, contextual DJ commentary.
    
    Args:
        segments: List of segment info with song metadata
        context: User-provided DJ context (theme, mood, etc.)
        frequency: How often to add comments (minimal, moderate, frequent)
    
    Returns:
        List of creative DJ comments
    """
    client = get_azure_openai_client()
    
    if not client:
        log("[AZURE_DJ] GPT not available, using fallback commentary")
        return generate_fallback_commentary(segments, context)
    
    # Build song list description - human-friendly, no technical details
    songs_desc = []
    for i, seg in enumerate(segments):
        meta = extract_song_metadata(seg)
        lang_info = LANGUAGE_INFO.get(meta.language, {"country": "worldwide", "vibe": "great music"})
        
        song_line = f"{i+1}. \"{meta.title}\" ({meta.language.title()}"
        if meta.artist:
            song_line += f", by {meta.artist}"
        # Skip BPM - keep it human-like, not technical
        if meta.famous_actors:
            song_line += f", featuring: {', '.join(meta.famous_actors)}"
        song_line += ")"
        songs_desc.append(song_line)
    
    songs_list = "\n".join(songs_desc)
    
    # Build the prompt
    mood_str = context.get_mood_str() if hasattr(context, 'get_mood_str') else context.mood
    
    # Build shoutout instructions if we have custom shoutouts
    # Extract individual names from shoutouts (they might be full sentences)
    raw_shoutouts = context.custom_shoutouts if context.custom_shoutouts else []
    shoutout_names = []
    
    for item in raw_shoutouts:
        # Check if item looks like a sentence (contains common words or is long)
        if len(item) > 30 or any(word in item.lower() for word in ['noise', 'make', 'for', 'the', 'to', 'happy', 'year']):
            # Try to extract names from sentence like "Make some noise for Karim, Doni, Halima..."
            import re
            # Look for capitalized words that could be names
            potential_names = re.findall(r'\b([A-Z][a-z]+(?:\s[A-Z][a-z]+)?)\b', item)
            # Filter out common words
            common_words = {'Happy', 'New', 'Year', 'Make', 'Some', 'Noise', 'The', 'And', 'For', 'Let', 'Go'}
            names = [n for n in potential_names if n not in common_words and len(n) > 2]
            shoutout_names.extend(names)
        else:
            # Looks like a simple name
            shoutout_names.append(item)
    
    # Remove duplicates while preserving order
    seen = set()
    unique_names = []
    for name in shoutout_names:
        if name not in seen:
            seen.add(name)
            unique_names.append(name)
    shoutout_names = unique_names
    
    shoutout_list = ', '.join(shoutout_names) if shoutout_names else "the crowd"
    log(f"[AZURE_DJ] Shoutout names extracted: {shoutout_names}")
    
    # Extract unique languages from segments for cultural phrases
    unique_languages = set()
    for seg in segments:
        lang = seg.get('language', 'english').lower()
        if lang and lang != 'unknown':
            unique_languages.add(lang)
    log(f"[AZURE_DJ] Languages in playlist: {unique_languages}")
    
    # Calculate comment distribution based on number of songs
    num_songs = len(segments)
    
    # Distribution strategy - MORE FREQUENT for better coverage:
    # - 1 intro (theme-based)
    # - 1 outro (theme-based) 
    # - Every song gets a "next up" callout (except first)
    # - At least 1 shoutout per song if we have names
    # - Every song gets a cultural phrase  
    # - Every song gets a "hype" at 75% in to fill gaps
    num_next_up = max(1, num_songs - 1)  # All songs except first get next_up
    num_shoutouts = min(len(shoutout_names), num_songs) if shoutout_names else 0  # One per name, max one per song
    num_cultural = num_songs  # Every song gets a cultural callout
    num_hype = num_songs  # Every song gets a hype callout at 75% in
    
    total_comments = 2 + num_next_up + num_shoutouts + num_cultural + num_hype  # intro + outro + rest
    # Cap at 4x songs to allow for all the comments
    total_comments = min(total_comments, num_songs * 4)
    
    log(f"[AZURE_DJ] Comment distribution: intro=1, next_up={num_next_up}, shoutouts={num_shoutouts}, cultural={num_cultural}, hype={num_hype}, outro=1, total={total_comments}")
    
    # Build explicit segment assignment for each song to ensure coverage
    song_assignments = []
    for i in range(num_songs):
        song_num = i + 1
        song_title = segments[i].get('song_title', f'Song {song_num}')[:30]
        lang = segments[i].get('language', 'english')
        assignments = []
        if i == 0:
            assignments.append("intro")
        if i > 0:
            assignments.append("next_up")
        assignments.append("cultural @30%")
        if shoutout_names and i < len(shoutout_names):
            assignments.append(f"shoutout @50% for {shoutout_names[i]}")
        assignments.append("hype @75%")  # Every song gets hype at 75%
        if i == num_songs - 1:
            assignments.append("outro")
        song_assignments.append(f"  Song {song_num} ({song_title}, {lang}): {', '.join(assignments)}")
    
    assignment_text = "\\n".join(song_assignments)
    
    prompt = f"""Generate EXACTLY {total_comments} DJ voice-over comments as a JSON array.

PARTY: {context.theme}
MOOD: {mood_str}
PEOPLE: {shoutout_list}
{f"NOTES: {context.special_notes}" if context.special_notes else ""}

PLAYLIST:
{songs_list}

REQUIRED DISTRIBUTION:
{assignment_text}

COMMENT TYPES:
- intro: Theme opener (8-12 words), segment_index=0
- next_up: "Next up, [artist]!" (5-8 words), before each song
- cultural: Culture phrase (2-4 words): "Jhakaas!" (Hindi), "Vera level!" (Tamil), "Let's go!" (English)
- shoutout: Personal callout (5-10 words): "[Name], you rock!"  
- hype: Energy booster (3-6 words): "Keep it going!", "Feel the energy!"
- outro: Theme closing (8-12 words), segment_index={num_songs - 1}

OUTPUT FORMAT - Return ONLY this JSON array with {total_comments} objects:
[
  {{"type": "intro", "text": "Your intro text here", "segment_index": 0}},
  {{"type": "cultural", "text": "Jhakaas!", "segment_index": 0}},
  {{"type": "shoutout", "text": "Karim, you rock!", "segment_index": 0}},
  {{"type": "hype", "text": "Keep it going!", "segment_index": 0}},
  {{"type": "next_up", "text": "Next up, MJ!", "segment_index": 1}},
  ... more comments ...
  {{"type": "outro", "text": "Your outro text here", "segment_index": {num_songs - 1}}}
]

Generate the JSON array now:"""

    try:
        log(f"[AZURE_DJ] Generating {total_comments} creative comments with GPT...")
        
        response = client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "You are an energetic party DJ. Output ONLY a valid JSON array of objects. Each object must have 'type', 'text', and 'segment_index' keys. No other text."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.8,  # Slightly lower for more consistent format
            max_tokens=2000  # More tokens for longer lists
        )
        
        content = response.choices[0].message.content.strip()
        
        # Parse JSON from response
        # Sometimes GPT adds markdown code blocks
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        content = content.strip()
        
        comments_data = json.loads(content)
        
        # Debug: log raw parsed data type
        log(f"[AZURE_DJ] Parsed JSON type: {type(comments_data)}, len: {len(comments_data) if isinstance(comments_data, list) else 'N/A'}")
        
        comments = []
        for i, item in enumerate(comments_data):
            # Debug: log each item type
            if not isinstance(item, dict):
                log(f"[AZURE_DJ] WARNING: Item {i} is {type(item).__name__}: {str(item)[:100]}")
                continue
            comment = CreativeDJComment(
                text=item.get("text", ""),
                comment_type=item.get("type", "hype"),
                position="between" if item.get("type") not in ["intro", "outro"] else ("before" if item.get("type") == "intro" else "after"),
                segment_index=item.get("segment_index", 0)
            )
            comments.append(comment)
        
        log(f"[AZURE_DJ] Generated {len(comments)} creative comments!")
        for c in comments:
            log(f"[AZURE_DJ]   [{c.comment_type}] {c.text[:50]}...")
        
        return comments
        
    except json.JSONDecodeError as e:
        log(f"[AZURE_DJ] Failed to parse GPT response as JSON: {e}")
        log(f"[AZURE_DJ] Response was: {content[:500]}")
        return generate_fallback_commentary(segments, context)
    except Exception as e:
        log(f"[AZURE_DJ] GPT generation failed: {e}")
        return generate_fallback_commentary(segments, context)


def generate_fallback_commentary(
    segments: List[Dict],
    context: DJContext
) -> List[CreativeDJComment]:
    """Fallback commentary when GPT is not available - includes shoutouts and cultural callouts."""
    comments = []
    
    # Intro - theme-based
    intro_text = f"What's up party people! Welcome to {context.theme}! Let's get this celebration started!"
    comments.append(CreativeDJComment(
        text=intro_text,
        comment_type="intro",
        position="before",
        segment_index=0
    ))
    
    # Extract shoutout names from context
    shoutout_names = []
    for item in context.custom_shoutouts:
        import re
        if len(item) > 30 or any(word in item.lower() for word in ['noise', 'make', 'for', 'the']):
            potential_names = re.findall(r'\b([A-Z][a-z]+)\b', item)
            common_words = {'Happy', 'New', 'Year', 'Make', 'Some', 'Noise', 'The', 'And', 'For', 'Let', 'Go'}
            names = [n for n in potential_names if n not in common_words and len(n) > 2]
            shoutout_names.extend(names)
        else:
            shoutout_names.append(item)
    
    # Language-based cultural phrases
    cultural_phrases = {
        'hindi': ['Arey waah!', 'Ekdum mast!', 'Jhakaas!'],
        'tamil': ['Mass!', 'Theri!', 'Vera level!'],
        'malayalam': ['Adipoli!', 'Pwoli!', 'Kidu!'],
        'arabic': ['Yalla habibi!', 'Khalas!'],
        'turkish': ['Harika!', 'Süper!'],
        'english': ['Let\'s go!', 'Fire!', 'Vibes!']
    }
    
    num_segments = len(segments)
    shoutout_idx = 0
    
    # Add shoutouts and cultural callouts distributed across songs
    for i, seg in enumerate(segments):
        lang = seg.get('language', 'english').lower()
        
        # Add a cultural phrase for every other song (at 30% in)
        if i % 2 == 0 and lang in cultural_phrases:
            phrases = cultural_phrases.get(lang, cultural_phrases['english'])
            phrase = phrases[i % len(phrases)]
            comments.append(CreativeDJComment(
                text=phrase,
                comment_type="cultural",
                position="between",
                segment_index=i
            ))
        
        # Add a shoutout for every 3rd song (at 50% in)
        if shoutout_names and i % 3 == 1:
            name = shoutout_names[shoutout_idx % len(shoutout_names)]
            shoutout_idx += 1
            shoutout_text = f"{name}, this one's for you!"
            comments.append(CreativeDJComment(
                text=shoutout_text,
                comment_type="shoutout",
                position="between",
                segment_index=i
            ))
    
    # Middle transition comment (halfway through)
    if num_segments > 3:
        mid_idx = num_segments // 2
        mid_lang = segments[mid_idx].get("language", "english")
        mid_text = f"Halfway there! {mid_lang.title()} vibes coming in hot!"
        comments.append(CreativeDJComment(
            text=mid_text,
            comment_type="transition",
            position="between",
            segment_index=mid_idx
        ))
    
    # Outro - theme-based
    outro_text = f"That's a wrap! {context.theme} - what a night! Stay blessed!"
    comments.append(CreativeDJComment(
        text=outro_text,
        comment_type="outro",
        position="after",
        segment_index=num_segments - 1
    ))
    
    log(f"[AZURE_DJ] Fallback generated {len(comments)} comments with shoutouts and cultural callouts")
    return comments


async def generate_voice_clip_azure_async(
    text: str,
    output_path: Path,
    voice: str = "alloy"
) -> bool:
    """Generate a voice clip using Azure OpenAI gpt-4o-mini-audio-preview."""
    client = get_azure_openai_client()
    
    if not client:
        log("[AZURE_DJ] Azure OpenAI client not available for TTS")
        return False
    
    azure_voice = AZURE_DJ_VOICES.get(voice, "alloy")
    
    try:
        log(f"[AZURE_DJ] Generating voice with Azure OpenAI ({azure_voice})...")
        
        response = client.chat.completions.create(
            model=AZURE_OPENAI_AUDIO_DEPLOYMENT,  # gpt-4o-mini-audio-preview
            modalities=["text", "audio"],
            audio={"voice": azure_voice, "format": "wav"},
            messages=[
                {
                    "role": "system",
                    "content": "You are an energetic party DJ. Speak with enthusiasm and energy!"
                },
                {
                    "role": "user",
                    "content": f"Read this DJ announcement with high energy: {text}"
                }
            ]
        )
        
        # Extract audio data from response
        if response.choices[0].message.audio:
            audio_data = response.choices[0].message.audio.data
            wav_bytes = base64.b64decode(audio_data)
            
            with open(output_path, "wb") as f:
                f.write(wav_bytes)
            
            log(f"[AZURE_DJ] Generated voice clip: {output_path} ({len(wav_bytes)} bytes)")
            return True
        else:
            log("[AZURE_DJ] No audio data in response")
            return False
            
    except Exception as e:
        log(f"[AZURE_DJ] Azure OpenAI TTS failed: {e}")
        return False


def generate_voice_clip_azure(
    text: str,
    output_path: Path,
    voice: str = "alloy"
) -> bool:
    """Synchronous wrapper for Azure voice generation."""
    import asyncio
    
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(generate_voice_clip_azure_async(text, output_path, voice))
    
    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor() as pool:
        future = pool.submit(asyncio.run, generate_voice_clip_azure_async(text, output_path, voice))
        return future.result()


async def generate_voice_clip_edge_tts_async(
    text: str,
    output_path: Path,
    voice_id: str = "en-US-GuyNeural"
) -> bool:
    """Fallback to edge-tts for voice generation."""
    if not EDGE_TTS_AVAILABLE:
        return False
    
    try:
        communicate = edge_tts.Communicate(text, voice_id)
        await communicate.save(str(output_path))
        return True
    except Exception as e:
        log(f"[AZURE_DJ] edge-tts failed: {e}")
        return False


def generate_voice_clip_edge_tts(
    text: str,
    output_path: Path,
    voice_id: str = "en-US-GuyNeural"
) -> bool:
    """Synchronous wrapper for edge-tts."""
    import asyncio
    
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(generate_voice_clip_edge_tts_async(text, output_path, voice_id))
    
    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor() as pool:
        future = pool.submit(asyncio.run, generate_voice_clip_edge_tts_async(text, output_path, voice_id))
        return future.result()


def generate_voice_clip(
    text: str,
    output_path: Path,
    voice: str = "energetic_male"
) -> bool:
    """Generate voice clip using Azure OpenAI or fallback to edge-tts."""
    
    log(f"[AZURE_DJ] Generating voice clip for: '{text[:60]}...' -> {output_path}")
    
    # Try Azure OpenAI first
    if AZURE_OPENAI_AVAILABLE:
        log("[AZURE_DJ] Attempting Azure OpenAI TTS...")
        success = generate_voice_clip_azure(text, output_path, voice)
        if success:
            log(f"[AZURE_DJ] Azure OpenAI TTS SUCCESS: {output_path}")
            # Verify the file was created and has content
            if output_path.exists() and output_path.stat().st_size > 1000:
                log(f"[AZURE_DJ] File verified: {output_path.stat().st_size} bytes")
                return True
            else:
                log(f"[AZURE_DJ] WARNING: File too small or missing: {output_path}")
        else:
            log("[AZURE_DJ] Azure OpenAI TTS failed, trying edge-tts fallback...")
    else:
        log("[AZURE_DJ] Azure OpenAI not available, using edge-tts...")
    
    # Fallback to edge-tts
    if EDGE_TTS_AVAILABLE:
        log("[AZURE_DJ] Attempting edge-tts fallback...")
        edge_voice_map = {
            "energetic_male": "en-US-GuyNeural",
            "energetic_female": "en-US-AriaNeural",
            "deep_male": "en-US-ChristopherNeural",
            "party_female": "en-US-JennyNeural",
            "hype_male": "en-GB-RyanNeural",
        }
        voice_id = edge_voice_map.get(voice, "en-US-GuyNeural")
        success = generate_voice_clip_edge_tts(text, output_path, voice_id)
        if success:
            log(f"[AZURE_DJ] edge-tts SUCCESS: {output_path}")
            if output_path.exists() and output_path.stat().st_size > 1000:
                return True
        log("[AZURE_DJ] edge-tts also failed!")
    else:
        log("[AZURE_DJ] edge-tts not available!")
    
    log("[AZURE_DJ] All TTS engines failed - no voice clip generated!")
    return False


def get_dj_clip_duration(audio_path: str) -> float:
    """Get duration of a DJ audio clip."""
    try:
        cmd = [
            'ffprobe', '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            audio_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            return float(result.stdout.strip())
    except:
        pass
    return 2.0  # Default estimate


def get_stream_durations(video_path) -> Tuple[float, float]:
    """Get both video and audio stream durations separately."""
    cmd = [
        'ffprobe', '-v', 'error',
        '-show_entries', 'stream=codec_type,duration',
        '-of', 'json',
        str(video_path)
    ]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            data = json.loads(result.stdout)
            video_dur = 0
            audio_dur = 0
            for stream in data.get("streams", []):
                if stream.get("codec_type") == "video":
                    video_dur = float(stream.get("duration", 0))
                elif stream.get("codec_type") == "audio":
                    audio_dur = float(stream.get("duration", 0))
            return video_dur, audio_dur
    except Exception as e:
        log(f"[AZURE_DJ] Could not get stream durations: {e}")
    return 0, 0


def add_creative_dj_commentary_to_video(
    video_path: Path,
    segments: List[Dict],
    output_path: Path,
    context: DJContext,
    voice: str = "energetic_male",
    frequency: str = "moderate",
    progress_callback: callable = None
) -> Tuple[bool, List[Dict]]:
    """
    Complete creative DJ voice integration using Azure OpenAI.
    
    Args:
        video_path: Input video file
        segments: List of segment info with song metadata
        output_path: Output video file
        context: User-provided DJ context
        voice: Voice style to use
        frequency: Comment frequency
        progress_callback: Optional callback(step: str, detail: str) for progress updates
    
    Returns:
        Tuple of (success: bool, timeline: list of timing info)
    """
    timeline = []
    
    def report_progress(step: str, detail: str = ""):
        if progress_callback:
            progress_callback(step, detail)
        log(f"[AZURE_DJ] {step}" + (f": {detail}" if detail else ""))
    
    log("=" * 60)
    log("[AZURE_DJ] ADD_CREATIVE_DJ_COMMENTARY_TO_VIDEO STARTED")
    log(f"[AZURE_DJ] Theme: {context.theme}")
    log(f"[AZURE_DJ] Mood: {context.mood}")
    log(f"[AZURE_DJ] Voice: {voice}, Frequency: {frequency}")
    log(f"[AZURE_DJ] Segments: {len(segments)}")
    log(f"[AZURE_DJ] Azure OpenAI Available: {AZURE_OPENAI_AVAILABLE}")
    log("=" * 60)
    
    if not AZURE_OPENAI_AVAILABLE and not EDGE_TTS_AVAILABLE:
        log("[AZURE_DJ] No TTS engine available!")
        return False, timeline
    
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # Get video duration
        report_progress("Analyzing video", "Getting duration...")
        video_dur, audio_dur = get_stream_durations(video_path)
        video_duration = min(video_dur, audio_dur) if video_dur > 0 and audio_dur > 0 else max(video_dur, audio_dur)
        
        if video_duration <= 0:
            video_duration = get_dj_clip_duration(str(video_path))
        
        log(f"[AZURE_DJ] Video duration: {video_duration:.2f}s")
        
        # Generate creative commentary using GPT
        report_progress("Generating DJ script", "AI is writing your commentary...")
        comments = generate_creative_commentary_with_gpt(segments, context, frequency)
        
        if not comments:
            log("[AZURE_DJ] No comments generated!")
            return False, timeline
        
        report_progress("Planning DJ moments", f"Prepared {len(comments)} commentary spots")
        
        # Build segment timing map - use actual video_start_time if available
        # This tells us exactly when each song starts in the final video
        num_segments = len(segments)
        segment_timings = []  # (start_time_in_video, duration, song_title)
        
        for i, seg in enumerate(segments):
            if 'video_start_time' in seg:
                # Use actual timing from exporter
                start = seg['video_start_time']
                duration = seg.get('segment_duration', 60)
            else:
                # Fallback: estimate based on position (4s intro + segments)
                avg_seg_duration = (video_duration - 4 - 3) / num_segments if num_segments > 0 else 60
                start = 4.0 + (i * avg_seg_duration)
                duration = avg_seg_duration
            
            segment_timings.append({
                'start': start,
                'duration': duration,
                'title': seg.get('song_title', seg.get('title', 'Unknown')),
                'artist': seg.get('artist', '')
            })
        
        log(f"[AZURE_DJ] Segment timings in final video:")
        for i, st in enumerate(segment_timings):
            log(f"[AZURE_DJ]   Song {i+1}: {st['title'][:30]} @ {st['start']:.1f}s ({st['duration']:.0f}s)")
        
        dj_clips = []
        clip_times = []  # Track time per clip for ETA
        # Track last end time to prevent overlaps
        last_clip_end_time = 0.0
        
        for i, comment in enumerate(comments):
            clip_path = temp_dir / f"dj_{i}.wav"
            seg_idx = min(comment.segment_index, num_segments - 1)
            
            # Calculate start time based on comment type using actual segment timings
            if comment.comment_type == "intro":
                start_time = 1.5  # Intro starts 1.5 seconds in (during intro clip)
            elif comment.comment_type == "outro":
                start_time = max(video_duration - 12.0, video_duration * 0.85)
            elif comment.comment_type == "next_up":
                # "Next up" plays RIGHT BEFORE the song starts (during transition)
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    # Start 3 seconds before the song begins
                    start_time = max(song_start - 3.0, last_clip_end_time + 2.0)
                else:
                    start_time = last_clip_end_time + 5.0
            elif comment.comment_type == "shoutout":
                # Personal shoutouts go in the MIDDLE of the song (40-60% in)
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    song_duration = segment_timings[seg_idx]['duration']
                    # Place at 50% into the song
                    start_time = song_start + (song_duration * 0.5)
                else:
                    start_time = last_clip_end_time + 8.0
            elif comment.comment_type == "cultural":
                # Cultural phrases go early-mid in the song (25-35% in)
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    song_duration = segment_timings[seg_idx]['duration']
                    # Place at 30% into the song
                    start_time = song_start + (song_duration * 0.3)
                else:
                    start_time = last_clip_end_time + 6.0
            elif comment.comment_type == "song_intro":
                # Song intro should play RIGHT AT the start of that song
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    # Start DJ comment 2 seconds into the song (after transition settles)
                    start_time = song_start + 2.0
                else:
                    start_time = last_clip_end_time + 5.0
            elif comment.comment_type == "transition":
                # Transition comment plays BEFORE the next song starts (during crossfade)
                if seg_idx < len(segment_timings):
                    next_song_start = segment_timings[seg_idx]['start']
                    # Start 5 seconds before the next song
                    start_time = max(next_song_start - 5.0, last_clip_end_time + 3.0)
                else:
                    start_time = last_clip_end_time + 5.0
            elif comment.comment_type == "hype" or comment.comment_type == "peak_energy":
                # Hype/peak energy comments go LATE in the song (75% in)
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    song_duration = segment_timings[seg_idx]['duration']
                    # Place at 75% into the song (fills gap before next song)
                    start_time = song_start + (song_duration * 0.75)
                else:
                    start_time = last_clip_end_time + 8.0
            else:
                # Other comments - position 30% into the segment
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    song_duration = segment_timings[seg_idx]['duration']
                    start_time = song_start + (song_duration * 0.3)
                else:
                    start_time = last_clip_end_time + 5.0
            
            # Ensure no overlap with previous clip (at least 3s gap)
            if start_time < last_clip_end_time + 3.0:
                start_time = last_clip_end_time + 3.0
            
            # Generate voice clip with progress reporting
            comment_label = comment.comment_type.replace('_', ' ').title()
            
            # Calculate ETA for voice recording
            remaining_clips = len(comments) - i
            avg_clip_time = sum(clip_times) / len(clip_times) if clip_times else 3.0  # ~3s per clip
            eta_seconds = int(remaining_clips * avg_clip_time)
            eta_str = f"~{eta_seconds}s" if eta_seconds < 60 else f"~{eta_seconds//60}m {eta_seconds%60}s"
            
            report_progress(f"Recording voice ({i+1}/{len(comments)}) [{eta_str}]", f"{comment_label}: \"{comment.text[:40]}...\"")
            
            clip_start_time = time.time()
            log(f"[AZURE_DJ] Generating clip {i+1}/{len(comments)}: {comment.comment_type}")
            success = generate_voice_clip(comment.text, clip_path, voice)
            clip_times.append(time.time() - clip_start_time)  # Track time for ETA
            
            if success and clip_path.exists():
                clip_duration = get_dj_clip_duration(str(clip_path))
                
                # Ensure clip doesn't extend past video
                if start_time + clip_duration > video_duration:
                    start_time = max(0, video_duration - clip_duration - 2)
                
                # Ensure no overlap with previous clip (at least 2s gap)
                if start_time < last_clip_end_time + 2.0:
                    start_time = last_clip_end_time + 2.0
                
                dj_clips.append({
                    "path": str(clip_path),
                    "start_time": start_time,
                    "duration": clip_duration,
                    "type": comment.comment_type,
                    "text": comment.text,
                })
                timeline.append({
                    "type": comment.comment_type,
                    "start_time": start_time,
                    "end_time": start_time + clip_duration,
                    "text": comment.text  # Full text for timeline display
                })
                
                # Track end time for next clip
                last_clip_end_time = start_time + clip_duration
                
                log(f"[AZURE_DJ]   ✓ {comment.comment_type} @ {start_time:.1f}s ({clip_duration:.1f}s)")
            else:
                log(f"[AZURE_DJ]   ✗ Failed to generate clip for {comment.comment_type}")
        
        if not dj_clips:
            log("[AZURE_DJ] No DJ clips generated!")
            return False, timeline
        
        # Mix DJ audio with video using FFmpeg
        report_progress("Mixing DJ voice", f"Blending {len(dj_clips)} voice clips with music...")
        log(f"[AZURE_DJ] Mixing {len(dj_clips)} clips with video...")
        
        # Build FFmpeg command
        input_args = ['-i', str(video_path)]
        filter_parts = []
        
        # Build duck expression
        duck_expr_parts = []
        for clip in dj_clips:
            start = clip["start_time"]
            end = start + clip["duration"]
            # Add 0.5s fade-in/out buffer for smoother ducking
            duck_expr_parts.append(f"between(t,{start - 0.3},{end + 0.3})")
        
        # Music: duck to 20% during DJ (much quieter so DJ is clearly audible)
        # Also apply a gentle limiter to prevent clipping
        if duck_expr_parts:
            duck_cond = '+'.join(duck_expr_parts)
            # Duck music to 20% (was 35%) and add slight compression for consistent levels
            filter_parts.append(f"[0:a]volume='if({duck_cond},0.20,1.0)':eval=frame,alimiter=limit=0.95[music]")
        else:
            filter_parts.append("[0:a]anull[music]")
        
        # Add each DJ clip - boost voice to be VERY clear over ducked music
        # Apply normalization and boost to DJ clips
        mix_labels = ['[music]']
        for i, clip in enumerate(dj_clips):
            input_args.extend(['-i', clip["path"]])
            input_idx = i + 1
            delay_ms = int(clip["start_time"] * 1000)
            # Boost DJ voice to 3.0x (was 2.0x), normalize, and add slight compression
            filter_parts.append(f"[{input_idx}:a]adelay={delay_ms}|{delay_ms},volume=3.0,alimiter=limit=0.95[dj{i}]")
            mix_labels.append(f'[dj{i}]')
        
        # Final mix - heavily favor DJ voice
        num_inputs = len(mix_labels)
        # Music gets weight 1, each DJ clip gets weight 4 (was 2.5)
        weights = '1 ' + ' '.join(['4'] * (num_inputs - 1))
        filter_parts.append(
            f"{''.join(mix_labels)}amix=inputs={num_inputs}:duration=first:dropout_transition=0:normalize=0:weights='{weights}'[aout]"
        )
        
        filter_complex = ';'.join(filter_parts)
        
        cmd = [
            'ffmpeg', '-y',
            *input_args,
            '-filter_complex', filter_complex,
            '-map', '0:v',
            '-map', '[aout]',
            '-c:v', 'libx264', '-preset', 'fast',
            '-c:a', 'aac', '-b:a', '192k',
            '-vsync', 'cfr', '-r', '30',
            '-shortest',
            str(output_path)
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            log(f"[AZURE_DJ] FFmpeg failed: {result.stderr[:500]}")
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False, timeline
        
        log(f"[AZURE_DJ] Successfully created DJ video: {output_path}")
        log("[AZURE_DJ] === DJ VOICE TIMELINE ===")
        for t in timeline:
            log(f"[AZURE_DJ]   {t['type'].upper():12} @ {t['start_time']:.1f}s - {t['end_time']:.1f}s")
        log("[AZURE_DJ] =========================")
        
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        
        return True, timeline
        
    except Exception as e:
        log(f"[AZURE_DJ] Error: {e}")
        import traceback
        traceback.print_exc()
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        return False, timeline


# Default DJ context for New Year 2025 party
DEFAULT_DJ_CONTEXT = DJContext(
    theme="New Year 2025 Party - Welcoming 2026!",
    mood="energetic, celebratory, festive, countdown vibes",
    audience="party guests ready to dance and celebrate",
    special_notes="Last party of 2025! Let's make it epic!",
    custom_shoutouts=["Happy New Year!", "2026 here we come!"]
)
