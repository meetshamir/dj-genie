"""
Azure OpenAI DJ Voice Service - Uses Azure OpenAI for creative DJ commentary and TTS.
Replaces edge-tts with Azure OpenAI's gpt-4o-mini-audio-preview for high-quality voice.
Uses GPT-4 to generate creative, contextual DJ commentary based on user theme and song metadata.
"""

import os
import base64
import logging
import subprocess
import tempfile
import json
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Tuple
import sys

logger = logging.getLogger(__name__)

def log(msg: str):
    """Print with immediate flush for logging."""
    print(msg, flush=True)
    sys.stdout.flush()

# Azure OpenAI configuration - Using AAD authentication (DefaultAzureCredential)
AZURE_OPENAI_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT", "https://aidj-openai.openai.azure.com/")
AZURE_OPENAI_DEPLOYMENT = os.environ.get("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
AZURE_OPENAI_AUDIO_DEPLOYMENT = os.environ.get("AZURE_OPENAI_AUDIO_DEPLOYMENT", "gpt-4o-mini-audio")

# Check if Azure OpenAI is configured (using AAD auth - no API key needed!)
AZURE_OPENAI_AVAILABLE = bool(AZURE_OPENAI_ENDPOINT)
_azure_credential = None
_AzureOpenAI = None

if AZURE_OPENAI_AVAILABLE:
    try:
        from openai import AzureOpenAI as _AzureOpenAI
        from azure.identity import DefaultAzureCredential, get_bearer_token_provider
        _azure_credential = DefaultAzureCredential()
        log("[AZURE_DJ] Azure OpenAI configured with AAD authentication (DefaultAzureCredential)")
    except ImportError as e:
        AZURE_OPENAI_AVAILABLE = False
        log(f"[AZURE_DJ] Required packages not installed: {e}")
        log("[AZURE_DJ] Run: pip install openai azure-identity")
else:
    log("[AZURE_DJ] Azure OpenAI not configured (missing AZURE_OPENAI_ENDPOINT)")

# Fallback to edge-tts if Azure OpenAI is not available
try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
except ImportError:
    EDGE_TTS_AVAILABLE = False

# DJ Voice options for Azure OpenAI (Alloy, Echo, Shimmer are supported)
AZURE_DJ_VOICES = {
    "energetic_male": "echo",      # Echo has an energetic male quality
    "energetic_female": "shimmer", # Shimmer is energetic female
    "deep_male": "echo",           # Echo for deep male (adjust with prompting)
    "party_female": "alloy",       # Alloy is versatile
    "hype_male": "echo",           # Echo for hype
}

# Language metadata for creative commentary
LANGUAGE_INFO = {
    "english": {"country": "worldwide", "vibe": "global hits", "artists": ["Ed Sheeran", "Taylor Swift", "Bruno Mars"]},
    "hindi": {"country": "India", "vibe": "Bollywood magic", "artists": ["Arijit Singh", "Shreya Ghoshal", "SRK movies"]},
    "malayalam": {"country": "Kerala", "vibe": "Mollywood melodies", "artists": ["Mohanlal", "Mammootty", "Dulquer"]},
    "tamil": {"country": "Tamil Nadu", "vibe": "Kollywood beats", "artists": ["Rajinikanth", "Vijay", "AR Rahman"]},
    "turkish": {"country": "Turkey", "vibe": "Turkish pop vibes", "artists": ["Tarkan", "Sezen Aksu"]},
    "uzbek": {"country": "Uzbekistan", "vibe": "Central Asian rhythms", "artists": ["Uzbek folk fusion"]},
    "arabic": {"country": "Middle East", "vibe": "Arabic grooves", "artists": ["Amr Diab", "Nancy Ajram"]},
}


@dataclass
class DJContext:
    """User-provided context for DJ commentary."""
    theme: str = "New Year 2025 Party - Welcoming 2026!"
    mood: str = "energetic, celebratory, festive"
    audience: str = "party guests ready to dance"
    special_notes: str = ""
    custom_shoutouts: List[str] = field(default_factory=list)


@dataclass
class SongMetadata:
    """Extracted metadata about a song for DJ commentary."""
    title: str
    artist: Optional[str] = None
    language: str = "english"
    bpm: Optional[float] = None
    energy_score: float = 0.5
    movie_or_album: Optional[str] = None
    famous_actors: List[str] = field(default_factory=list)
    position: int = 0


@dataclass
class CreativeDJComment:
    """A creative DJ comment generated by GPT."""
    text: str
    comment_type: str  # intro, hype, transition, language_switch, song_intro, outro
    position: str  # before, after, between
    segment_index: int
    audio_path: Optional[str] = None
    voice_style: str = "energetic"


def get_azure_openai_client():
    """Get Azure OpenAI client if available, using AAD authentication."""
    global _azure_credential, _AzureOpenAI
    
    if not AZURE_OPENAI_AVAILABLE or not _AzureOpenAI or not _azure_credential:
        return None
    
    try:
        from azure.identity import get_bearer_token_provider
        token_provider = get_bearer_token_provider(
            _azure_credential,
            "https://cognitiveservices.azure.com/.default"
        )
        
        client = _AzureOpenAI(
            api_version="2025-01-01-preview",
            azure_endpoint=AZURE_OPENAI_ENDPOINT,
            azure_ad_token_provider=token_provider
        )
        return client
    except Exception as e:
        log(f"[AZURE_DJ] Failed to create Azure OpenAI client: {e}")
        return None


def extract_song_metadata(song_info: Dict) -> SongMetadata:
    """Extract useful metadata from song info for creative commentary."""
    title = song_info.get("song_title", "Unknown Track")
    artist = song_info.get("artist")
    language = song_info.get("language", "english").lower()
    bpm = song_info.get("bpm")
    energy = song_info.get("energy_score", 0.5)
    position = song_info.get("position", 0)
    
    # Try to extract movie/actor info from title
    movie_or_album = None
    famous_actors = []
    
    # Common patterns in Bollywood/Indian movie song titles
    title_lower = title.lower()
    
    # Check for common actors/stars in title
    star_keywords = {
        "srk": "Shah Rukh Khan",
        "shah rukh": "Shah Rukh Khan",
        "shahrukh": "Shah Rukh Khan",
        "salman": "Salman Khan",
        "aamir": "Aamir Khan",
        "ranveer": "Ranveer Singh",
        "ranbir": "Ranbir Kapoor",
        "hrithik": "Hrithik Roshan",
        "deepika": "Deepika Padukone",
        "alia": "Alia Bhatt",
        "priyanka": "Priyanka Chopra",
        "rajini": "Rajinikanth",
        "vijay": "Vijay",
        "mohanlal": "Mohanlal",
        "mammootty": "Mammootty",
    }
    
    for keyword, star_name in star_keywords.items():
        if keyword in title_lower:
            famous_actors.append(star_name)
    
    return SongMetadata(
        title=title,
        artist=artist,
        language=language,
        bpm=bpm,
        energy_score=energy,
        movie_or_album=movie_or_album,
        famous_actors=famous_actors,
        position=position
    )


def generate_creative_commentary_with_gpt(
    segments: List[Dict],
    context: DJContext,
    frequency: str = "moderate"
) -> List[CreativeDJComment]:
    """
    Use Azure OpenAI GPT to generate creative, contextual DJ commentary.
    
    Args:
        segments: List of segment info with song metadata
        context: User-provided DJ context (theme, mood, etc.)
        frequency: How often to add comments (minimal, moderate, frequent)
    
    Returns:
        List of creative DJ comments
    """
    client = get_azure_openai_client()
    
    if not client:
        log("[AZURE_DJ] GPT not available, using fallback commentary")
        return generate_fallback_commentary(segments, context)
    
    # Build song list description
    songs_desc = []
    for i, seg in enumerate(segments):
        meta = extract_song_metadata(seg)
        lang_info = LANGUAGE_INFO.get(meta.language, {"country": "worldwide", "vibe": "great music"})
        
        song_line = f"{i+1}. \"{meta.title}\" ({meta.language.title()}"
        if meta.artist:
            song_line += f", by {meta.artist}"
        if meta.bpm:
            song_line += f", {int(meta.bpm)} BPM"
        song_line += f", energy: {int(meta.energy_score * 100)}%"
        if meta.famous_actors:
            song_line += f", featuring: {', '.join(meta.famous_actors)}"
        song_line += ")"
        songs_desc.append(song_line)
    
    songs_list = "\n".join(songs_desc)
    
    # Determine number of comments based on frequency
    comment_counts = {
        "minimal": 3,      # intro, mid, outro
        "moderate": 5,     # intro, 2 middle, language switch, outro
        "frequent": 7,     # More comments
        "maximum": 10      # Many comments
    }
    num_comments = comment_counts.get(frequency, 5)
    
    # Build the prompt
    prompt = f"""You are an energetic AI DJ at a party! Generate {num_comments} DJ voice-over comments for a video DJ mix.

PARTY THEME: {context.theme}
MOOD: {context.mood}
AUDIENCE: {context.audience}
{f"SPECIAL NOTES: {context.special_notes}" if context.special_notes else ""}
{f"SHOUTOUTS TO INCLUDE: {', '.join(context.custom_shoutouts)}" if context.custom_shoutouts else ""}

PLAYLIST ({len(segments)} songs):
{songs_list}

INSTRUCTIONS:
1. Create exactly {num_comments} DJ comments as JSON array
2. Be creative, hype, and reference the party theme (New Year 2026!)
3. When songs are from Bollywood/Indian cinema, reference famous actors/movies
4. Use phrases like "groove like SRK", "Bollywood swag", "Kerala vibes", etc.
5. Include language-specific shoutouts (Hindi: "Arey waah!", Tamil: "Thalaivar style!", etc.)
6. Reference BPM changes and energy levels when transitioning
7. Keep each comment SHORT (1-2 sentences, under 15 seconds to speak)
8. For New Year theme: countdown references, "2026 here we come!", "last party of 2025!"

OUTPUT FORMAT (JSON array):
[
  {{"type": "intro", "text": "Your intro comment", "segment_index": 0}},
  {{"type": "transition", "text": "Transition comment", "segment_index": 1}},
  ...
  {{"type": "outro", "text": "Closing comment", "segment_index": {len(segments)-1}}}
]

Types: intro, hype, transition, language_switch, song_intro, peak_energy, outro

Generate the JSON array now:"""

    try:
        log(f"[AZURE_DJ] Generating {num_comments} creative comments with GPT...")
        
        response = client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "You are an energetic party DJ. Output only valid JSON arrays."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.9,  # High creativity
            max_tokens=1500
        )
        
        content = response.choices[0].message.content.strip()
        
        # Parse JSON from response
        # Sometimes GPT adds markdown code blocks
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        content = content.strip()
        
        comments_data = json.loads(content)
        
        comments = []
        for item in comments_data:
            comment = CreativeDJComment(
                text=item.get("text", ""),
                comment_type=item.get("type", "hype"),
                position="between" if item.get("type") not in ["intro", "outro"] else ("before" if item.get("type") == "intro" else "after"),
                segment_index=item.get("segment_index", 0)
            )
            comments.append(comment)
        
        log(f"[AZURE_DJ] Generated {len(comments)} creative comments!")
        for c in comments:
            log(f"[AZURE_DJ]   [{c.comment_type}] {c.text[:50]}...")
        
        return comments
        
    except json.JSONDecodeError as e:
        log(f"[AZURE_DJ] Failed to parse GPT response as JSON: {e}")
        log(f"[AZURE_DJ] Response was: {content[:500]}")
        return generate_fallback_commentary(segments, context)
    except Exception as e:
        log(f"[AZURE_DJ] GPT generation failed: {e}")
        return generate_fallback_commentary(segments, context)


def generate_fallback_commentary(
    segments: List[Dict],
    context: DJContext
) -> List[CreativeDJComment]:
    """Fallback commentary when GPT is not available."""
    comments = []
    
    # Intro
    intro_text = f"What's up party people! Welcome to the {context.theme}! Let's get this celebration started!"
    comments.append(CreativeDJComment(
        text=intro_text,
        comment_type="intro",
        position="before",
        segment_index=0
    ))
    
    # Middle comment
    if len(segments) > 1:
        mid_idx = len(segments) // 2
        mid_lang = segments[mid_idx].get("language", "english")
        mid_text = f"We're halfway through this amazing mix! {mid_lang.title()} vibes coming in hot!"
        comments.append(CreativeDJComment(
            text=mid_text,
            comment_type="transition",
            position="between",
            segment_index=mid_idx
        ))
    
    # Outro
    outro_text = f"That's a wrap on this incredible party! {context.theme} - what a night! Stay groovy!"
    comments.append(CreativeDJComment(
        text=outro_text,
        comment_type="outro",
        position="after",
        segment_index=len(segments) - 1
    ))
    
    return comments


async def generate_voice_clip_azure_async(
    text: str,
    output_path: Path,
    voice: str = "alloy"
) -> bool:
    """Generate a voice clip using Azure OpenAI gpt-4o-mini-audio-preview."""
    client = get_azure_openai_client()
    
    if not client:
        log("[AZURE_DJ] Azure OpenAI client not available for TTS")
        return False
    
    azure_voice = AZURE_DJ_VOICES.get(voice, "alloy")
    
    try:
        log(f"[AZURE_DJ] Generating voice with Azure OpenAI ({azure_voice})...")
        
        response = client.chat.completions.create(
            model=AZURE_OPENAI_AUDIO_DEPLOYMENT,  # gpt-4o-mini-audio-preview
            modalities=["text", "audio"],
            audio={"voice": azure_voice, "format": "wav"},
            messages=[
                {
                    "role": "system",
                    "content": "You are an energetic party DJ. Speak with enthusiasm and energy!"
                },
                {
                    "role": "user",
                    "content": f"Read this DJ announcement with high energy: {text}"
                }
            ]
        )
        
        # Extract audio data from response
        if response.choices[0].message.audio:
            audio_data = response.choices[0].message.audio.data
            wav_bytes = base64.b64decode(audio_data)
            
            with open(output_path, "wb") as f:
                f.write(wav_bytes)
            
            log(f"[AZURE_DJ] Generated voice clip: {output_path} ({len(wav_bytes)} bytes)")
            return True
        else:
            log("[AZURE_DJ] No audio data in response")
            return False
            
    except Exception as e:
        log(f"[AZURE_DJ] Azure OpenAI TTS failed: {e}")
        return False


def generate_voice_clip_azure(
    text: str,
    output_path: Path,
    voice: str = "alloy"
) -> bool:
    """Synchronous wrapper for Azure voice generation."""
    import asyncio
    
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(generate_voice_clip_azure_async(text, output_path, voice))
    
    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor() as pool:
        future = pool.submit(asyncio.run, generate_voice_clip_azure_async(text, output_path, voice))
        return future.result()


async def generate_voice_clip_edge_tts_async(
    text: str,
    output_path: Path,
    voice_id: str = "en-US-GuyNeural"
) -> bool:
    """Fallback to edge-tts for voice generation."""
    if not EDGE_TTS_AVAILABLE:
        return False
    
    try:
        communicate = edge_tts.Communicate(text, voice_id)
        await communicate.save(str(output_path))
        return True
    except Exception as e:
        log(f"[AZURE_DJ] edge-tts failed: {e}")
        return False


def generate_voice_clip_edge_tts(
    text: str,
    output_path: Path,
    voice_id: str = "en-US-GuyNeural"
) -> bool:
    """Synchronous wrapper for edge-tts."""
    import asyncio
    
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(generate_voice_clip_edge_tts_async(text, output_path, voice_id))
    
    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor() as pool:
        future = pool.submit(asyncio.run, generate_voice_clip_edge_tts_async(text, output_path, voice_id))
        return future.result()


def generate_voice_clip(
    text: str,
    output_path: Path,
    voice: str = "energetic_male"
) -> bool:
    """Generate voice clip using Azure OpenAI or fallback to edge-tts."""
    
    # Try Azure OpenAI first
    if AZURE_OPENAI_AVAILABLE:
        success = generate_voice_clip_azure(text, output_path, voice)
        if success:
            return True
        log("[AZURE_DJ] Azure OpenAI TTS failed, trying fallback...")
    
    # Fallback to edge-tts
    if EDGE_TTS_AVAILABLE:
        edge_voice_map = {
            "energetic_male": "en-US-GuyNeural",
            "energetic_female": "en-US-AriaNeural",
            "deep_male": "en-US-ChristopherNeural",
            "party_female": "en-US-JennyNeural",
            "hype_male": "en-GB-RyanNeural",
        }
        voice_id = edge_voice_map.get(voice, "en-US-GuyNeural")
        return generate_voice_clip_edge_tts(text, output_path, voice_id)
    
    log("[AZURE_DJ] No TTS engine available!")
    return False


def get_dj_clip_duration(audio_path: str) -> float:
    """Get duration of a DJ audio clip."""
    try:
        cmd = [
            'ffprobe', '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            audio_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            return float(result.stdout.strip())
    except:
        pass
    return 2.0  # Default estimate


def get_stream_durations(video_path) -> Tuple[float, float]:
    """Get both video and audio stream durations separately."""
    cmd = [
        'ffprobe', '-v', 'error',
        '-show_entries', 'stream=codec_type,duration',
        '-of', 'json',
        str(video_path)
    ]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            data = json.loads(result.stdout)
            video_dur = 0
            audio_dur = 0
            for stream in data.get("streams", []):
                if stream.get("codec_type") == "video":
                    video_dur = float(stream.get("duration", 0))
                elif stream.get("codec_type") == "audio":
                    audio_dur = float(stream.get("duration", 0))
            return video_dur, audio_dur
    except Exception as e:
        log(f"[AZURE_DJ] Could not get stream durations: {e}")
    return 0, 0


def add_creative_dj_commentary_to_video(
    video_path: Path,
    segments: List[Dict],
    output_path: Path,
    context: DJContext,
    voice: str = "energetic_male",
    frequency: str = "moderate"
) -> Tuple[bool, List[Dict]]:
    """
    Complete creative DJ voice integration using Azure OpenAI.
    
    Args:
        video_path: Input video file
        segments: List of segment info with song metadata
        output_path: Output video file
        context: User-provided DJ context
        voice: Voice style to use
        frequency: Comment frequency
    
    Returns:
        Tuple of (success: bool, timeline: list of timing info)
    """
    timeline = []
    
    log("=" * 60)
    log("[AZURE_DJ] ADD_CREATIVE_DJ_COMMENTARY_TO_VIDEO STARTED")
    log(f"[AZURE_DJ] Theme: {context.theme}")
    log(f"[AZURE_DJ] Mood: {context.mood}")
    log(f"[AZURE_DJ] Voice: {voice}, Frequency: {frequency}")
    log(f"[AZURE_DJ] Segments: {len(segments)}")
    log(f"[AZURE_DJ] Azure OpenAI Available: {AZURE_OPENAI_AVAILABLE}")
    log("=" * 60)
    
    if not AZURE_OPENAI_AVAILABLE and not EDGE_TTS_AVAILABLE:
        log("[AZURE_DJ] No TTS engine available!")
        return False, timeline
    
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # Get video duration
        video_dur, audio_dur = get_stream_durations(video_path)
        video_duration = min(video_dur, audio_dur) if video_dur > 0 and audio_dur > 0 else max(video_dur, audio_dur)
        
        if video_duration <= 0:
            video_duration = get_dj_clip_duration(str(video_path))
        
        log(f"[AZURE_DJ] Video duration: {video_duration:.2f}s")
        
        # Generate creative commentary using GPT
        comments = generate_creative_commentary_with_gpt(segments, context, frequency)
        
        if not comments:
            log("[AZURE_DJ] No comments generated!")
            return False, timeline
        
        # Calculate timing for each comment
        num_segments = len(segments)
        segment_duration = video_duration / num_segments if num_segments > 0 else video_duration
        
        dj_clips = []
        for i, comment in enumerate(comments):
            clip_path = temp_dir / f"dj_{i}.wav"
            
            # Calculate start time based on comment type and segment index
            if comment.comment_type == "intro":
                start_time = 2.0  # 2 seconds in
            elif comment.comment_type == "outro":
                start_time = max(video_duration - 8.0, video_duration * 0.85)
            else:
                # Position at the start of the target segment
                seg_idx = min(comment.segment_index, num_segments - 1)
                start_time = seg_idx * segment_duration + 1.0  # 1 second into segment
            
            # Generate voice clip
            log(f"[AZURE_DJ] Generating clip {i+1}/{len(comments)}: {comment.comment_type}")
            success = generate_voice_clip(comment.text, clip_path, voice)
            
            if success and clip_path.exists():
                clip_duration = get_dj_clip_duration(str(clip_path))
                
                # Ensure clip doesn't extend past video
                if start_time + clip_duration > video_duration:
                    start_time = max(0, video_duration - clip_duration - 1)
                
                dj_clips.append({
                    "path": str(clip_path),
                    "start_time": start_time,
                    "duration": clip_duration,
                    "type": comment.comment_type,
                    "text": comment.text,
                })
                timeline.append({
                    "type": comment.comment_type,
                    "start_time": start_time,
                    "end_time": start_time + clip_duration,
                    "text": comment.text[:50] + "..."
                })
                log(f"[AZURE_DJ]   ✓ {comment.comment_type} @ {start_time:.1f}s ({clip_duration:.1f}s)")
            else:
                log(f"[AZURE_DJ]   ✗ Failed to generate clip for {comment.comment_type}")
        
        if not dj_clips:
            log("[AZURE_DJ] No DJ clips generated!")
            return False, timeline
        
        # Mix DJ audio with video using FFmpeg
        log(f"[AZURE_DJ] Mixing {len(dj_clips)} clips with video...")
        
        # Build FFmpeg command
        input_args = ['-i', str(video_path)]
        filter_parts = []
        
        # Build duck expression
        duck_expr_parts = []
        for clip in dj_clips:
            start = clip["start_time"]
            end = start + clip["duration"]
            duck_expr_parts.append(f"between(t,{start},{end})")
        
        # Music: duck to 15% during DJ
        if duck_expr_parts:
            duck_cond = '+'.join(duck_expr_parts)
            filter_parts.append(f"[0:a]volume='if({duck_cond},0.15,1.0)':eval=frame[music]")
        else:
            filter_parts.append("[0:a]anull[music]")
        
        # Add each DJ clip
        mix_labels = ['[music]']
        for i, clip in enumerate(dj_clips):
            input_args.extend(['-i', clip["path"]])
            input_idx = i + 1
            delay_ms = int(clip["start_time"] * 1000)
            filter_parts.append(f"[{input_idx}:a]adelay={delay_ms}|{delay_ms},volume=3.0[dj{i}]")
            mix_labels.append(f'[dj{i}]')
        
        # Final mix
        num_inputs = len(mix_labels)
        weights = '1 ' + ' '.join(['3'] * (num_inputs - 1))
        filter_parts.append(
            f"{''.join(mix_labels)}amix=inputs={num_inputs}:duration=first:dropout_transition=0:normalize=0:weights='{weights}'[aout]"
        )
        
        filter_complex = ';'.join(filter_parts)
        
        cmd = [
            'ffmpeg', '-y',
            *input_args,
            '-filter_complex', filter_complex,
            '-map', '0:v',
            '-map', '[aout]',
            '-c:v', 'libx264', '-preset', 'fast',
            '-c:a', 'aac', '-b:a', '192k',
            '-vsync', 'cfr', '-r', '30',
            '-shortest',
            str(output_path)
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            log(f"[AZURE_DJ] FFmpeg failed: {result.stderr[:500]}")
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False, timeline
        
        log(f"[AZURE_DJ] Successfully created DJ video: {output_path}")
        log("[AZURE_DJ] === DJ VOICE TIMELINE ===")
        for t in timeline:
            log(f"[AZURE_DJ]   {t['type'].upper():12} @ {t['start_time']:.1f}s - {t['end_time']:.1f}s")
        log("[AZURE_DJ] =========================")
        
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        
        return True, timeline
        
    except Exception as e:
        log(f"[AZURE_DJ] Error: {e}")
        import traceback
        traceback.print_exc()
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        return False, timeline


# Default DJ context for New Year 2025 party
DEFAULT_DJ_CONTEXT = DJContext(
    theme="New Year 2025 Party - Welcoming 2026!",
    mood="energetic, celebratory, festive, countdown vibes",
    audience="party guests ready to dance and celebrate",
    special_notes="Last party of 2025! Let's make it epic!",
    custom_shoutouts=["Happy New Year!", "2026 here we come!"]
)
