"""
Azure OpenAI DJ Voice Service - Uses Azure OpenAI for creative DJ commentary and TTS.
Replaces edge-tts with Azure OpenAI's gpt-4o-mini-audio-preview for high-quality voice.
Uses GPT-4 to generate creative, contextual DJ commentary based on user theme and song metadata.
"""

import os
import base64
import logging
import subprocess
import tempfile
import json
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Tuple
import sys

logger = logging.getLogger(__name__)

def log(msg: str):
    """Print with immediate flush for logging."""
    print(msg, flush=True)
    sys.stdout.flush()

# Azure OpenAI configuration - Using AAD authentication (DefaultAzureCredential)
AZURE_OPENAI_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT", "https://aidj-openai.openai.azure.com/")
AZURE_OPENAI_DEPLOYMENT = os.environ.get("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
AZURE_OPENAI_AUDIO_DEPLOYMENT = os.environ.get("AZURE_OPENAI_AUDIO_DEPLOYMENT", "gpt-4o-mini-audio")

# Check if Azure OpenAI is configured (using AAD auth - no API key needed!)
AZURE_OPENAI_AVAILABLE = bool(AZURE_OPENAI_ENDPOINT)
_azure_credential = None
_AzureOpenAI = None

if AZURE_OPENAI_AVAILABLE:
    try:
        from openai import AzureOpenAI as _AzureOpenAI
        from azure.identity import DefaultAzureCredential, get_bearer_token_provider
        _azure_credential = DefaultAzureCredential()
        log("[AZURE_DJ] Azure OpenAI configured with AAD authentication (DefaultAzureCredential)")
    except ImportError as e:
        AZURE_OPENAI_AVAILABLE = False
        log(f"[AZURE_DJ] Required packages not installed: {e}")
        log("[AZURE_DJ] Run: pip install openai azure-identity")
else:
    log("[AZURE_DJ] Azure OpenAI not configured (missing AZURE_OPENAI_ENDPOINT)")

# Fallback to edge-tts if Azure OpenAI is not available
try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
except ImportError:
    EDGE_TTS_AVAILABLE = False

# DJ Voice options for Azure OpenAI (Alloy, Echo, Shimmer are supported)
AZURE_DJ_VOICES = {
    "energetic_male": "echo",      # Echo has an energetic male quality
    "energetic_female": "shimmer", # Shimmer is energetic female
    "deep_male": "echo",           # Echo for deep male (adjust with prompting)
    "party_female": "alloy",       # Alloy is versatile
    "hype_male": "echo",           # Echo for hype
}

# Language metadata for creative commentary
LANGUAGE_INFO = {
    "english": {"country": "worldwide", "vibe": "global hits", "artists": ["Ed Sheeran", "Taylor Swift", "Bruno Mars"]},
    "hindi": {"country": "India", "vibe": "Bollywood magic", "artists": ["Arijit Singh", "Shreya Ghoshal", "SRK movies"]},
    "malayalam": {"country": "Kerala", "vibe": "Mollywood melodies", "artists": ["Mohanlal", "Mammootty", "Dulquer"]},
    "tamil": {"country": "Tamil Nadu", "vibe": "Kollywood beats", "artists": ["Rajinikanth", "Vijay", "AR Rahman"]},
    "turkish": {"country": "Turkey", "vibe": "Turkish pop vibes", "artists": ["Tarkan", "Sezen Aksu"]},
    "uzbek": {"country": "Uzbekistan", "vibe": "Central Asian rhythms", "artists": ["Uzbek folk fusion"]},
    "arabic": {"country": "Middle East", "vibe": "Arabic grooves", "artists": ["Amr Diab", "Nancy Ajram"]},
}


@dataclass
class DJContext:
    """User-provided context for DJ commentary."""
    theme: str = "New Year 2025 Party - Welcoming 2026!"
    mood: str = "energetic, celebratory, festive"  # Can be string or list
    audience: str = "party guests ready to dance"
    special_notes: str = ""
    custom_shoutouts: List[str] = field(default_factory=list)
    original_prompt: str = ""  # The original user prompt for reference
    
    def get_mood_str(self) -> str:
        """Get mood as a string, handling both string and list inputs."""
        if isinstance(self.mood, list):
            return ", ".join(self.mood)
        return self.mood


@dataclass
class SongMetadata:
    """Extracted metadata about a song for DJ commentary."""
    title: str
    artist: Optional[str] = None
    language: str = "english"
    bpm: Optional[float] = None
    energy_score: float = 0.5
    movie_or_album: Optional[str] = None
    famous_actors: List[str] = field(default_factory=list)
    position: int = 0


@dataclass
class CreativeDJComment:
    """A creative DJ comment generated by GPT."""
    text: str
    comment_type: str  # intro, hype, transition, language_switch, song_intro, outro
    position: str  # before, after, between
    segment_index: int
    audio_path: Optional[str] = None
    voice_style: str = "energetic"


def get_azure_openai_client():
    """Get Azure OpenAI client if available, using AAD authentication."""
    global _azure_credential, _AzureOpenAI
    
    if not AZURE_OPENAI_AVAILABLE or not _AzureOpenAI or not _azure_credential:
        return None
    
    try:
        from azure.identity import get_bearer_token_provider
        token_provider = get_bearer_token_provider(
            _azure_credential,
            "https://cognitiveservices.azure.com/.default"
        )
        
        client = _AzureOpenAI(
            api_version="2025-01-01-preview",
            azure_endpoint=AZURE_OPENAI_ENDPOINT,
            azure_ad_token_provider=token_provider
        )
        return client
    except Exception as e:
        log(f"[AZURE_DJ] Failed to create Azure OpenAI client: {e}")
        return None


def extract_song_metadata(song_info: Dict) -> SongMetadata:
    """Extract useful metadata from song info for creative commentary."""
    title = song_info.get("song_title", "Unknown Track")
    artist = song_info.get("artist")
    language = song_info.get("language", "english").lower()
    bpm = song_info.get("bpm")
    energy = song_info.get("energy_score", 0.5)
    position = song_info.get("position", 0)
    
    # Try to extract movie/actor info from title
    movie_or_album = None
    famous_actors = []
    
    # Common patterns in Bollywood/Indian movie song titles
    title_lower = title.lower()
    
    # Check for common actors/stars in title
    star_keywords = {
        "srk": "Shah Rukh Khan",
        "shah rukh": "Shah Rukh Khan",
        "shahrukh": "Shah Rukh Khan",
        "salman": "Salman Khan",
        "aamir": "Aamir Khan",
        "ranveer": "Ranveer Singh",
        "ranbir": "Ranbir Kapoor",
        "hrithik": "Hrithik Roshan",
        "deepika": "Deepika Padukone",
        "alia": "Alia Bhatt",
        "priyanka": "Priyanka Chopra",
        "rajini": "Rajinikanth",
        "vijay": "Vijay",
        "mohanlal": "Mohanlal",
        "mammootty": "Mammootty",
    }
    
    for keyword, star_name in star_keywords.items():
        if keyword in title_lower:
            famous_actors.append(star_name)
    
    return SongMetadata(
        title=title,
        artist=artist,
        language=language,
        bpm=bpm,
        energy_score=energy,
        movie_or_album=movie_or_album,
        famous_actors=famous_actors,
        position=position
    )


def generate_creative_commentary_with_gpt(
    segments: List[Dict],
    context: DJContext,
    frequency: str = "moderate"
) -> List[CreativeDJComment]:
    """
    Use Azure OpenAI GPT to generate creative, contextual DJ commentary.
    
    Args:
        segments: List of segment info with song metadata
        context: User-provided DJ context (theme, mood, etc.)
        frequency: How often to add comments (minimal, moderate, frequent)
    
    Returns:
        List of creative DJ comments
    """
    client = get_azure_openai_client()
    
    if not client:
        log("[AZURE_DJ] GPT not available, using fallback commentary")
        return generate_fallback_commentary(segments, context)
    
    # Build song list description - human-friendly, no technical details
    songs_desc = []
    for i, seg in enumerate(segments):
        meta = extract_song_metadata(seg)
        lang_info = LANGUAGE_INFO.get(meta.language, {"country": "worldwide", "vibe": "great music"})
        
        song_line = f"{i+1}. \"{meta.title}\" ({meta.language.title()}"
        if meta.artist:
            song_line += f", by {meta.artist}"
        # Skip BPM - keep it human-like, not technical
        if meta.famous_actors:
            song_line += f", featuring: {', '.join(meta.famous_actors)}"
        song_line += ")"
        songs_desc.append(song_line)
    
    songs_list = "\n".join(songs_desc)
    
    # Build the prompt
    mood_str = context.get_mood_str() if hasattr(context, 'get_mood_str') else context.mood
    
    # Build shoutout instructions if we have custom shoutouts
    shoutout_names = context.custom_shoutouts if context.custom_shoutouts else []
    shoutout_list = ', '.join(shoutout_names) if shoutout_names else "the crowd"
    
    # Calculate comment distribution based on number of songs
    num_songs = len(segments)
    
    # Distribution strategy:
    # - 1 intro (theme-based)
    # - 1 outro (theme-based) 
    # - ~40% of songs get a "next up" callout
    # - ~30% of songs get a personal shoutout
    # - ~30% of songs get a cultural phrase
    num_next_up = max(1, int(num_songs * 0.4))
    num_shoutouts = max(1, int(num_songs * 0.3)) if shoutout_names else 0
    num_cultural = max(1, int(num_songs * 0.3))
    
    total_comments = 2 + num_next_up + num_shoutouts + num_cultural  # intro + outro + rest
    total_comments = min(total_comments, num_songs + 2)  # Cap at songs + intro/outro
    
    prompt = f"""You are an energetic AI DJ at a party! Generate EXACTLY {total_comments} DJ voice-over comments.

=== PARTY INFO ===
THEME: {context.theme}
MOOD: {mood_str}
PEOPLE AT THE PARTY: {shoutout_list}
{f"SPECIAL NOTES: {context.special_notes}" if context.special_notes else ""}

=== PLAYLIST ({num_songs} songs) ===
{songs_list}

=== COMMENT TYPES (use these EXACT types) ===

1. **intro** (1 comment) - Theme-based party opener
   - Reference the theme: "{context.theme}"
   - 8-12 words max
   - Example: "Happy New Year everyone! 2026 here we come, let's party!"

2. **next_up** ({num_next_up} comments) - Quick song/artist callout BEFORE the song
   - MUST be 5-8 words only!
   - Just announce what's coming, nothing else
   - Examples: "Next up, MJ!", "Here comes Badshah!", "AR Rahman time, let's go!"

3. **shoutout** ({num_shoutouts} comments) - Personal callout to party people DURING a song
   - MUST include a name from: {shoutout_list}
   - 5-10 words max
   - Examples: "Muskaan, you're on fire!", "Karim, break a leg!", "Yo Shamir, this is your jam!"

4. **cultural** ({num_cultural} comments) - Cultural slang phrases DURING a song
   - MUST be 2-4 words ONLY! Super short!
   - Match the song's language/culture:
     * Hindi: "Arey waah!", "Jhakaas!", "Ekdum mast!"
     * Tamil: "Mass!", "Theri!", "Vera level!"
     * Malayalam: "Adipoli!", "Pwoli!", "Kidu!"
     * Punjabi: "Balle balle!", "Oye hoye!", "Paaji rocks!"
     * Arabic: "Yalla habibi!", "Khalas!"
     * Turkish: "Harika!", "Süper!"
     * English: "Let's go!", "Fire!", "Vibes!"

5. **outro** (1 comment) - Theme-based closing
   - Reference the theme again
   - 8-12 words max
   - Example: "What a night! Happy 2026 everyone, stay blessed!"

=== RULES ===
1. segment_index 0 = first song, 1 = second, etc.
2. "intro" MUST have segment_index: 0
3. "outro" MUST have segment_index: {num_songs - 1}
4. "next_up" goes BEFORE a song starts (segment_index = that song's index)
5. "shoutout" and "cultural" go DURING a song (segment_index = that song's index)
6. DISTRIBUTE evenly - don't cluster all comments at the start!
7. Use DIFFERENT names for each shoutout - spread the love!

=== OUTPUT FORMAT (JSON array) ===
[
  {{"type": "intro", "text": "Theme intro here", "segment_index": 0}},
  {{"type": "next_up", "text": "Next up, [artist]!", "segment_index": 2}},
  {{"type": "shoutout", "text": "[Name], you rock!", "segment_index": 3}},
  {{"type": "cultural", "text": "Adipoli!", "segment_index": 5}},
  {{"type": "next_up", "text": "Here comes MJ!", "segment_index": 6}},
  {{"type": "outro", "text": "Theme outro here", "segment_index": {num_songs - 1}}}
]

Generate EXACTLY {total_comments} comments now (1 intro + {num_next_up} next_up + {num_shoutouts} shoutout + {num_cultural} cultural + 1 outro):"""

    try:
        log(f"[AZURE_DJ] Generating {num_comments} creative comments with GPT...")
        
        response = client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "You are an energetic party DJ. Output only valid JSON arrays."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.9,  # High creativity
            max_tokens=1500
        )
        
        content = response.choices[0].message.content.strip()
        
        # Parse JSON from response
        # Sometimes GPT adds markdown code blocks
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        content = content.strip()
        
        comments_data = json.loads(content)
        
        comments = []
        for item in comments_data:
            comment = CreativeDJComment(
                text=item.get("text", ""),
                comment_type=item.get("type", "hype"),
                position="between" if item.get("type") not in ["intro", "outro"] else ("before" if item.get("type") == "intro" else "after"),
                segment_index=item.get("segment_index", 0)
            )
            comments.append(comment)
        
        log(f"[AZURE_DJ] Generated {len(comments)} creative comments!")
        for c in comments:
            log(f"[AZURE_DJ]   [{c.comment_type}] {c.text[:50]}...")
        
        return comments
        
    except json.JSONDecodeError as e:
        log(f"[AZURE_DJ] Failed to parse GPT response as JSON: {e}")
        log(f"[AZURE_DJ] Response was: {content[:500]}")
        return generate_fallback_commentary(segments, context)
    except Exception as e:
        log(f"[AZURE_DJ] GPT generation failed: {e}")
        return generate_fallback_commentary(segments, context)


def generate_fallback_commentary(
    segments: List[Dict],
    context: DJContext
) -> List[CreativeDJComment]:
    """Fallback commentary when GPT is not available."""
    comments = []
    
    # Intro
    intro_text = f"What's up party people! Welcome to the {context.theme}! Let's get this celebration started!"
    comments.append(CreativeDJComment(
        text=intro_text,
        comment_type="intro",
        position="before",
        segment_index=0
    ))
    
    # Middle comment
    if len(segments) > 1:
        mid_idx = len(segments) // 2
        mid_lang = segments[mid_idx].get("language", "english")
        mid_text = f"We're halfway through this amazing mix! {mid_lang.title()} vibes coming in hot!"
        comments.append(CreativeDJComment(
            text=mid_text,
            comment_type="transition",
            position="between",
            segment_index=mid_idx
        ))
    
    # Outro
    outro_text = f"That's a wrap on this incredible party! {context.theme} - what a night! Stay groovy!"
    comments.append(CreativeDJComment(
        text=outro_text,
        comment_type="outro",
        position="after",
        segment_index=len(segments) - 1
    ))
    
    return comments


async def generate_voice_clip_azure_async(
    text: str,
    output_path: Path,
    voice: str = "alloy"
) -> bool:
    """Generate a voice clip using Azure OpenAI gpt-4o-mini-audio-preview."""
    client = get_azure_openai_client()
    
    if not client:
        log("[AZURE_DJ] Azure OpenAI client not available for TTS")
        return False
    
    azure_voice = AZURE_DJ_VOICES.get(voice, "alloy")
    
    try:
        log(f"[AZURE_DJ] Generating voice with Azure OpenAI ({azure_voice})...")
        
        response = client.chat.completions.create(
            model=AZURE_OPENAI_AUDIO_DEPLOYMENT,  # gpt-4o-mini-audio-preview
            modalities=["text", "audio"],
            audio={"voice": azure_voice, "format": "wav"},
            messages=[
                {
                    "role": "system",
                    "content": "You are an energetic party DJ. Speak with enthusiasm and energy!"
                },
                {
                    "role": "user",
                    "content": f"Read this DJ announcement with high energy: {text}"
                }
            ]
        )
        
        # Extract audio data from response
        if response.choices[0].message.audio:
            audio_data = response.choices[0].message.audio.data
            wav_bytes = base64.b64decode(audio_data)
            
            with open(output_path, "wb") as f:
                f.write(wav_bytes)
            
            log(f"[AZURE_DJ] Generated voice clip: {output_path} ({len(wav_bytes)} bytes)")
            return True
        else:
            log("[AZURE_DJ] No audio data in response")
            return False
            
    except Exception as e:
        log(f"[AZURE_DJ] Azure OpenAI TTS failed: {e}")
        return False


def generate_voice_clip_azure(
    text: str,
    output_path: Path,
    voice: str = "alloy"
) -> bool:
    """Synchronous wrapper for Azure voice generation."""
    import asyncio
    
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(generate_voice_clip_azure_async(text, output_path, voice))
    
    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor() as pool:
        future = pool.submit(asyncio.run, generate_voice_clip_azure_async(text, output_path, voice))
        return future.result()


async def generate_voice_clip_edge_tts_async(
    text: str,
    output_path: Path,
    voice_id: str = "en-US-GuyNeural"
) -> bool:
    """Fallback to edge-tts for voice generation."""
    if not EDGE_TTS_AVAILABLE:
        return False
    
    try:
        communicate = edge_tts.Communicate(text, voice_id)
        await communicate.save(str(output_path))
        return True
    except Exception as e:
        log(f"[AZURE_DJ] edge-tts failed: {e}")
        return False


def generate_voice_clip_edge_tts(
    text: str,
    output_path: Path,
    voice_id: str = "en-US-GuyNeural"
) -> bool:
    """Synchronous wrapper for edge-tts."""
    import asyncio
    
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(generate_voice_clip_edge_tts_async(text, output_path, voice_id))
    
    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor() as pool:
        future = pool.submit(asyncio.run, generate_voice_clip_edge_tts_async(text, output_path, voice_id))
        return future.result()


def generate_voice_clip(
    text: str,
    output_path: Path,
    voice: str = "energetic_male"
) -> bool:
    """Generate voice clip using Azure OpenAI or fallback to edge-tts."""
    
    log(f"[AZURE_DJ] Generating voice clip for: '{text[:60]}...' -> {output_path}")
    
    # Try Azure OpenAI first
    if AZURE_OPENAI_AVAILABLE:
        log("[AZURE_DJ] Attempting Azure OpenAI TTS...")
        success = generate_voice_clip_azure(text, output_path, voice)
        if success:
            log(f"[AZURE_DJ] Azure OpenAI TTS SUCCESS: {output_path}")
            # Verify the file was created and has content
            if output_path.exists() and output_path.stat().st_size > 1000:
                log(f"[AZURE_DJ] File verified: {output_path.stat().st_size} bytes")
                return True
            else:
                log(f"[AZURE_DJ] WARNING: File too small or missing: {output_path}")
        else:
            log("[AZURE_DJ] Azure OpenAI TTS failed, trying edge-tts fallback...")
    else:
        log("[AZURE_DJ] Azure OpenAI not available, using edge-tts...")
    
    # Fallback to edge-tts
    if EDGE_TTS_AVAILABLE:
        log("[AZURE_DJ] Attempting edge-tts fallback...")
        edge_voice_map = {
            "energetic_male": "en-US-GuyNeural",
            "energetic_female": "en-US-AriaNeural",
            "deep_male": "en-US-ChristopherNeural",
            "party_female": "en-US-JennyNeural",
            "hype_male": "en-GB-RyanNeural",
        }
        voice_id = edge_voice_map.get(voice, "en-US-GuyNeural")
        success = generate_voice_clip_edge_tts(text, output_path, voice_id)
        if success:
            log(f"[AZURE_DJ] edge-tts SUCCESS: {output_path}")
            if output_path.exists() and output_path.stat().st_size > 1000:
                return True
        log("[AZURE_DJ] edge-tts also failed!")
    else:
        log("[AZURE_DJ] edge-tts not available!")
    
    log("[AZURE_DJ] All TTS engines failed - no voice clip generated!")
    return False


def get_dj_clip_duration(audio_path: str) -> float:
    """Get duration of a DJ audio clip."""
    try:
        cmd = [
            'ffprobe', '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            audio_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            return float(result.stdout.strip())
    except:
        pass
    return 2.0  # Default estimate


def get_stream_durations(video_path) -> Tuple[float, float]:
    """Get both video and audio stream durations separately."""
    cmd = [
        'ffprobe', '-v', 'error',
        '-show_entries', 'stream=codec_type,duration',
        '-of', 'json',
        str(video_path)
    ]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            data = json.loads(result.stdout)
            video_dur = 0
            audio_dur = 0
            for stream in data.get("streams", []):
                if stream.get("codec_type") == "video":
                    video_dur = float(stream.get("duration", 0))
                elif stream.get("codec_type") == "audio":
                    audio_dur = float(stream.get("duration", 0))
            return video_dur, audio_dur
    except Exception as e:
        log(f"[AZURE_DJ] Could not get stream durations: {e}")
    return 0, 0


def add_creative_dj_commentary_to_video(
    video_path: Path,
    segments: List[Dict],
    output_path: Path,
    context: DJContext,
    voice: str = "energetic_male",
    frequency: str = "moderate"
) -> Tuple[bool, List[Dict]]:
    """
    Complete creative DJ voice integration using Azure OpenAI.
    
    Args:
        video_path: Input video file
        segments: List of segment info with song metadata
        output_path: Output video file
        context: User-provided DJ context
        voice: Voice style to use
        frequency: Comment frequency
    
    Returns:
        Tuple of (success: bool, timeline: list of timing info)
    """
    timeline = []
    
    log("=" * 60)
    log("[AZURE_DJ] ADD_CREATIVE_DJ_COMMENTARY_TO_VIDEO STARTED")
    log(f"[AZURE_DJ] Theme: {context.theme}")
    log(f"[AZURE_DJ] Mood: {context.mood}")
    log(f"[AZURE_DJ] Voice: {voice}, Frequency: {frequency}")
    log(f"[AZURE_DJ] Segments: {len(segments)}")
    log(f"[AZURE_DJ] Azure OpenAI Available: {AZURE_OPENAI_AVAILABLE}")
    log("=" * 60)
    
    if not AZURE_OPENAI_AVAILABLE and not EDGE_TTS_AVAILABLE:
        log("[AZURE_DJ] No TTS engine available!")
        return False, timeline
    
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # Get video duration
        video_dur, audio_dur = get_stream_durations(video_path)
        video_duration = min(video_dur, audio_dur) if video_dur > 0 and audio_dur > 0 else max(video_dur, audio_dur)
        
        if video_duration <= 0:
            video_duration = get_dj_clip_duration(str(video_path))
        
        log(f"[AZURE_DJ] Video duration: {video_duration:.2f}s")
        
        # Generate creative commentary using GPT
        comments = generate_creative_commentary_with_gpt(segments, context, frequency)
        
        if not comments:
            log("[AZURE_DJ] No comments generated!")
            return False, timeline
        
        # Build segment timing map - use actual video_start_time if available
        # This tells us exactly when each song starts in the final video
        num_segments = len(segments)
        segment_timings = []  # (start_time_in_video, duration, song_title)
        
        for i, seg in enumerate(segments):
            if 'video_start_time' in seg:
                # Use actual timing from exporter
                start = seg['video_start_time']
                duration = seg.get('segment_duration', 60)
            else:
                # Fallback: estimate based on position (4s intro + segments)
                avg_seg_duration = (video_duration - 4 - 3) / num_segments if num_segments > 0 else 60
                start = 4.0 + (i * avg_seg_duration)
                duration = avg_seg_duration
            
            segment_timings.append({
                'start': start,
                'duration': duration,
                'title': seg.get('song_title', seg.get('title', 'Unknown')),
                'artist': seg.get('artist', '')
            })
        
        log(f"[AZURE_DJ] Segment timings in final video:")
        for i, st in enumerate(segment_timings):
            log(f"[AZURE_DJ]   Song {i+1}: {st['title'][:30]} @ {st['start']:.1f}s ({st['duration']:.0f}s)")
        
        dj_clips = []
        # Track last end time to prevent overlaps
        last_clip_end_time = 0.0
        
        for i, comment in enumerate(comments):
            clip_path = temp_dir / f"dj_{i}.wav"
            seg_idx = min(comment.segment_index, num_segments - 1)
            
            # Calculate start time based on comment type using actual segment timings
            if comment.comment_type == "intro":
                start_time = 1.5  # Intro starts 1.5 seconds in (during intro clip)
            elif comment.comment_type == "outro":
                start_time = max(video_duration - 12.0, video_duration * 0.85)
            elif comment.comment_type == "next_up":
                # "Next up" plays RIGHT BEFORE the song starts (during transition)
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    # Start 3 seconds before the song begins
                    start_time = max(song_start - 3.0, last_clip_end_time + 2.0)
                else:
                    start_time = last_clip_end_time + 5.0
            elif comment.comment_type == "shoutout":
                # Personal shoutouts go in the MIDDLE of the song (40-60% in)
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    song_duration = segment_timings[seg_idx]['duration']
                    # Place at 50% into the song
                    start_time = song_start + (song_duration * 0.5)
                else:
                    start_time = last_clip_end_time + 8.0
            elif comment.comment_type == "cultural":
                # Cultural phrases go early-mid in the song (25-35% in)
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    song_duration = segment_timings[seg_idx]['duration']
                    # Place at 30% into the song
                    start_time = song_start + (song_duration * 0.3)
                else:
                    start_time = last_clip_end_time + 6.0
            elif comment.comment_type == "song_intro":
                # Song intro should play RIGHT AT the start of that song
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    # Start DJ comment 2 seconds into the song (after transition settles)
                    start_time = song_start + 2.0
                else:
                    start_time = last_clip_end_time + 5.0
            elif comment.comment_type == "transition":
                # Transition comment plays BEFORE the next song starts (during crossfade)
                if seg_idx < len(segment_timings):
                    next_song_start = segment_timings[seg_idx]['start']
                    # Start 5 seconds before the next song
                    start_time = max(next_song_start - 5.0, last_clip_end_time + 3.0)
                else:
                    start_time = last_clip_end_time + 5.0
            elif comment.comment_type == "hype" or comment.comment_type == "peak_energy":
                # Hype/peak energy comments go in the MIDDLE of the song
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    song_duration = segment_timings[seg_idx]['duration']
                    # Place at 40% into the song
                    start_time = song_start + (song_duration * 0.4)
                else:
                    start_time = last_clip_end_time + 8.0
            else:
                # Other comments - position 30% into the segment
                if seg_idx < len(segment_timings):
                    song_start = segment_timings[seg_idx]['start']
                    song_duration = segment_timings[seg_idx]['duration']
                    start_time = song_start + (song_duration * 0.3)
                else:
                    start_time = last_clip_end_time + 5.0
            
            # Ensure no overlap with previous clip (at least 3s gap)
            if start_time < last_clip_end_time + 3.0:
                start_time = last_clip_end_time + 3.0
            
            # Generate voice clip
            log(f"[AZURE_DJ] Generating clip {i+1}/{len(comments)}: {comment.comment_type}")
            success = generate_voice_clip(comment.text, clip_path, voice)
            
            if success and clip_path.exists():
                clip_duration = get_dj_clip_duration(str(clip_path))
                
                # Ensure clip doesn't extend past video
                if start_time + clip_duration > video_duration:
                    start_time = max(0, video_duration - clip_duration - 2)
                
                # Ensure no overlap with previous clip (at least 2s gap)
                if start_time < last_clip_end_time + 2.0:
                    start_time = last_clip_end_time + 2.0
                
                dj_clips.append({
                    "path": str(clip_path),
                    "start_time": start_time,
                    "duration": clip_duration,
                    "type": comment.comment_type,
                    "text": comment.text,
                })
                timeline.append({
                    "type": comment.comment_type,
                    "start_time": start_time,
                    "end_time": start_time + clip_duration,
                    "text": comment.text[:50] + "..."
                })
                
                # Track end time for next clip
                last_clip_end_time = start_time + clip_duration
                
                log(f"[AZURE_DJ]   ✓ {comment.comment_type} @ {start_time:.1f}s ({clip_duration:.1f}s)")
            else:
                log(f"[AZURE_DJ]   ✗ Failed to generate clip for {comment.comment_type}")
        
        if not dj_clips:
            log("[AZURE_DJ] No DJ clips generated!")
            return False, timeline
        
        # Mix DJ audio with video using FFmpeg
        log(f"[AZURE_DJ] Mixing {len(dj_clips)} clips with video...")
        
        # Build FFmpeg command
        input_args = ['-i', str(video_path)]
        filter_parts = []
        
        # Build duck expression
        duck_expr_parts = []
        for clip in dj_clips:
            start = clip["start_time"]
            end = start + clip["duration"]
            # Add 0.5s fade-in/out buffer for smoother ducking
            duck_expr_parts.append(f"between(t,{start - 0.3},{end + 0.3})")
        
        # Music: duck to 20% during DJ (much quieter so DJ is clearly audible)
        # Also apply a gentle limiter to prevent clipping
        if duck_expr_parts:
            duck_cond = '+'.join(duck_expr_parts)
            # Duck music to 20% (was 35%) and add slight compression for consistent levels
            filter_parts.append(f"[0:a]volume='if({duck_cond},0.20,1.0)':eval=frame,alimiter=limit=0.95[music]")
        else:
            filter_parts.append("[0:a]anull[music]")
        
        # Add each DJ clip - boost voice to be VERY clear over ducked music
        # Apply normalization and boost to DJ clips
        mix_labels = ['[music]']
        for i, clip in enumerate(dj_clips):
            input_args.extend(['-i', clip["path"]])
            input_idx = i + 1
            delay_ms = int(clip["start_time"] * 1000)
            # Boost DJ voice to 3.0x (was 2.0x), normalize, and add slight compression
            filter_parts.append(f"[{input_idx}:a]adelay={delay_ms}|{delay_ms},volume=3.0,alimiter=limit=0.95[dj{i}]")
            mix_labels.append(f'[dj{i}]')
        
        # Final mix - heavily favor DJ voice
        num_inputs = len(mix_labels)
        # Music gets weight 1, each DJ clip gets weight 4 (was 2.5)
        weights = '1 ' + ' '.join(['4'] * (num_inputs - 1))
        filter_parts.append(
            f"{''.join(mix_labels)}amix=inputs={num_inputs}:duration=first:dropout_transition=0:normalize=0:weights='{weights}'[aout]"
        )
        
        filter_complex = ';'.join(filter_parts)
        
        cmd = [
            'ffmpeg', '-y',
            *input_args,
            '-filter_complex', filter_complex,
            '-map', '0:v',
            '-map', '[aout]',
            '-c:v', 'libx264', '-preset', 'fast',
            '-c:a', 'aac', '-b:a', '192k',
            '-vsync', 'cfr', '-r', '30',
            '-shortest',
            str(output_path)
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            log(f"[AZURE_DJ] FFmpeg failed: {result.stderr[:500]}")
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False, timeline
        
        log(f"[AZURE_DJ] Successfully created DJ video: {output_path}")
        log("[AZURE_DJ] === DJ VOICE TIMELINE ===")
        for t in timeline:
            log(f"[AZURE_DJ]   {t['type'].upper():12} @ {t['start_time']:.1f}s - {t['end_time']:.1f}s")
        log("[AZURE_DJ] =========================")
        
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        
        return True, timeline
        
    except Exception as e:
        log(f"[AZURE_DJ] Error: {e}")
        import traceback
        traceback.print_exc()
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        return False, timeline


# Default DJ context for New Year 2025 party
DEFAULT_DJ_CONTEXT = DJContext(
    theme="New Year 2025 Party - Welcoming 2026!",
    mood="energetic, celebratory, festive, countdown vibes",
    audience="party guests ready to dance and celebrate",
    special_notes="Last party of 2025! Let's make it epic!",
    custom_shoutouts=["Happy New Year!", "2026 here we come!"]
)
